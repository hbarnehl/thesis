{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import dateparser\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "### Some preliminary stuff\n",
    "\n",
    "# unify outlet names\n",
    "df.loc[df[\"page\"].str.startswith('Canal13'), \"page\"] = \"Canal13\"\n",
    "df.loc[df[\"page\"].str.startswith('100% Noticias'), \"page\"] = \"100% Noticias\"\n",
    "df.loc[df[\"page\"].str.startswith('Confidencial'), \"page\"] = \"Confidencial\"\n",
    "df.loc[df[\"page\"].str.startswith('Radio Corporacion'), \"page\"] = \"Radio Corporacion\"\n",
    "\n",
    "# extract canal13 dates \n",
    "df.loc[df[\"page\"] == 'Canal13', [\"date\"]] = df.loc[df[\"page\"] == 'Canal13',\n",
    "                                                 \"url\"].str.extract(r'(\\d\\d\\d\\d/\\d\\d/\\d\\d)').values\n",
    "# extract canal 6 dates\n",
    "df.loc[df[\"page\"] == 'Canal6', [\"date\"]] = df.loc[df[\"page\"] == 'Canal6',\n",
    "\n",
    "                                                \"url\"].str.extract(r'(\\d\\d\\d\\d/\\d\\d/\\d\\d)').values\n",
    "# extract radio 800 dates\n",
    "df.loc[df[\"page\"] == 'Radio 800', [\"date\"]] = df.loc[df[\"page\"] == 'Radio 800',\n",
    "                                                   \"url\"].str.extract(r'(\\d\\d\\d\\d/\\d\\d/\\d\\d)').values\n",
    "\n",
    "# convert canal10 date to datetime\n",
    "df10 = df.loc[df[\"page\"] == \"Canal10\"]\n",
    "df10.loc[:,'date'] = df10['date'].str.replace('de ', '', regex=True)\n",
    "df10.loc[:,'date'] = df10['date'].str.replace(r'Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday', '', regex=True)\n",
    "df10.loc[:,'date'] = pd.to_datetime(df10['date'])\n",
    "df.loc[df[\"page\"] == \"Canal10\", 'date'] = df10[\"date\"].to_list()\n",
    "\n",
    "# convert canal14 date to datetime\n",
    "# I am doing that with dateparser, because pandas cannot handle the spanish dates\n",
    "df14 = df.loc[df[\"page\"] == \"Canal14\"]\n",
    "df14.loc[:,'date'] = df14.loc[:,'date'].apply(lambda x: dateparser.parse(x, settings={'STRICT_PARSING': True}))\n",
    "df.loc[df[\"page\"] == \"Canal14\", 'date'] = df14[\"date\"].to_list()\n",
    "\n",
    "# 100 % noticias\n",
    "df100 = df.loc[df[\"page\"] == \"100% Noticias\"]\n",
    "# convert nan in date column to empty string so that dateparser works\n",
    "df100.fillna({\"date\":\" \"}, axis=0, inplace=True)\n",
    "df100.loc[:,'date'] = df100.loc[:,'date'].apply(lambda x: dateparser.parse(x, settings={'STRICT_PARSING': True}))\n",
    "df.loc[df[\"page\"] == \"100% Noticias\", 'date'] = df100[\"date\"].to_list()\n",
    "\n",
    "# canal2, canal4, confidencial, radio corporacion, radio nicaragua,\n",
    "# radio primerissima are already in datetime format\n",
    "\n",
    "# convert the rest to datetime\n",
    "df.loc[:,\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "# create year variable\n",
    "df[\"year\"] = pd.DatetimeIndex(df.date).year\n",
    "\n",
    "df.to_csv(\"dataset1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Tokenization Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for articles that do not have any text in them\n",
    "print(\"Number of articles without text per outlet\")\n",
    "print(df[df.text.isna()].groupby('page').size())\n",
    "\n",
    "# there are quite some articles without text. I randomly selected some of these to verify that\n",
    "# this is not a mistake in my crawler, but that they just don't have text. Indeed, they are all\n",
    "# articles that only contain videos. This is reflected by the fact that the outlets in thise list\n",
    "# are mostly TV and radio stations.\n",
    "\n",
    "print(\"\\nNumber of articles without text per year\")\n",
    "print(df[df.text.isna()].groupby('year').size())\n",
    "\n",
    "# Because the models need text to work, I will discard rows without text\n",
    "\n",
    "df = df.loc[~df.text.isna()].reset_index(drop = True)\n",
    "\n",
    "# removing boilerplate, news agency sources, hyperlinks etc.\n",
    "\n",
    "             # newlines, tabs etc.\n",
    "repl_dict = {r\"\\t|\\n|\\r|\\xa0\":\" \",\n",
    "             # whitespace\n",
    "             r\"\\s{2,}\":\" \",\n",
    "             # hashtags\n",
    "             \"#\": \"\",\n",
    "             # source in some articles\n",
    "            r'Fuente: El 19 Digital|Fuente: TN8':\"\",\n",
    "             # all hyperlinks\n",
    "            r\"http\\S+\":\"\",\n",
    "             # canal4 boilerplate\n",
    "            r\"Canal 4 Noticias[\\s\\S]+Canal 4 Nicaragua. Todos los derechos reservados\": \"\",\n",
    "            \"Comparte[.\\s]*?esto:[.\\s]*?Tweet[.\\s]*?WhatsApp[.\\s]*?Telegram\": \"\",\n",
    "            \"LEER TAMBIÉN\": \"\",\n",
    "            \"Leer más:\": \"\",\n",
    "            \"AMPLIACIÓN EN BREVE…\": \"\",\n",
    "             # news agency\n",
    "            \"(EFE)\":\"\",\n",
    "             # source mentioned at end of article\n",
    "            r\"Con información de\\:+$\": \"\",\n",
    "             # copyright stuff\n",
    "            \"© 100% Noticias ¡Con primicias a toda hora!\":\"\",\n",
    "            \"© Getty Images\":\"\",\n",
    "            r\"©\\s?[Vv]iva [Nn]icaragua,? (Canal 13 )?(Previous Next)?\": \"\",\n",
    "            \"© AFP\":\"\",\n",
    "            \"© AP\":\"\",\n",
    "            \"© creative commons\": \"\",\n",
    "            \"© El 19 Digital\": \"\",\n",
    "            \"© Consejo de Comunicación y Ciudadanía\": \"\",\n",
    "            \"© Juventud Presidente\": \"\",\n",
    "            \"© Ministerio de Gobernación\": \"\",\n",
    "             # copyrights for photographers\n",
    "            r\"\\w+?\\s+?\\w+?\\s+?©\":\"\",\n",
    "             # article suggestions\n",
    "            \"Te recomendamos:\": \"\",\n",
    "            \"Quizás te interesa:\":\"\",\n",
    "            \"Lee Aquí:\": \"\",\n",
    "             # twitter links\n",
    "            r\"pic\\.twitter\\.com.+?\\d{4}\": \"\",\n",
    "            r\"—.+?\\(@.+?\\).+?\\d{4}\":\"\",\n",
    "             # source information\n",
    "            r\"Con información de:.+$\": \"\"}\n",
    "\n",
    "df.replace({\"text\":repl_dict, \"title\":repl_dict}, regex=True, inplace = True)\n",
    "df\n",
    "\n",
    "df.to_csv(\"dataset_token_ready.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number of articles per outlet\n",
    "df.groupby('page').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save only text as csv file\n",
    "df = pd.read_csv(\"dataset_token_ready.csv\")[\"text\"]\n",
    "\n",
    "df.to_csv(\"texts.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text as generator\n",
    "csv_gen = (row for row in open(\"texts.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text as list\n",
    "textlist = pd.read_csv(\"dataset_token_ready.csv\")[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load spanish language model\n",
    "nlp = spacy.load('es_core_news_md', disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"ner\"])\n",
    "# add stopwords\n",
    "nlp.Defaults.stop_words |= {\"a\",\"y\", \"o\"}\n",
    "# reload language model to incorporate new stopwords \n",
    "nlp = spacy.load('es_core_news_md', disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"ner\"])\n",
    "\n",
    "tokenlist = []\n",
    "# go through rows of article texts\n",
    "for i, doc in enumerate(nlp.pipe(csv_gen, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"ner\"], n_process=4)):\n",
    "    # append lists of lemmatised tokens to tokenlist\n",
    "    tokenlist.append([token.lemma_ for token in doc if not token.is_stop and token.is_alpha])\n",
    "    if i % 10000 == 0 and i != 0: print(i)\n",
    "# remove first entry, which is the column title\n",
    "tokenlist = tokenlist[1:]\n",
    "# save to pickle\n",
    "with open(f\"tokens/tokens.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenlist, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate tokenlist into df, useful for later structural topic model and all subsampling\n",
    "# load tokenlist\n",
    "with open(\"tokens/tokens.pkl\", \"rb\") as f:\n",
    "    tokenlist = pickle.load(f)\n",
    "\n",
    "# integrate tokenlist back into dataframe\n",
    "df = pd.read_csv(\"dataset_token_ready.csv\")\n",
    "df = df.assign(tokens=tokenlist)\n",
    "\n",
    "# save dataset as pickle (csv does not work, because it cannot save the lists of tokens as cell entries)\n",
    "df.to_pickle(\"dataset_with_tokens.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
