{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import dateparser\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-60a6f40a3a3d>:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df10.loc[:,'date'] = df10['date'].str.replace('de ', '', regex=True)\n",
      "<ipython-input-14-60a6f40a3a3d>:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df10.loc[:,'date'] = df10['date'].str.replace(r'Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday', '', regex=True)\n",
      "<ipython-input-14-60a6f40a3a3d>:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df10.loc[:,'date'] = pd.to_datetime(df10['date'])\n",
      "/home/hennes/.local/lib/python3.8/site-packages/dateparser/date_parser.py:35: PytzUsageWarning: The localize method is no longer necessary, as this time zone supports the fold attribute (PEP 495). For more details on migrating to a PEP 495-compliant implementation, see https://pytz-deprecation-shim.readthedocs.io/en/latest/migration.html\n",
      "  date_obj = stz.localize(date_obj)\n",
      "<ipython-input-14-60a6f40a3a3d>:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df14.loc[:,'date'] = df14.loc[:,'date'].apply(lambda x: dateparser.parse(x, settings={'STRICT_PARSING': True}))\n",
      "<ipython-input-14-60a6f40a3a3d>:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df100.fillna({\"date\":\" \"}, axis=0, inplace=True)\n",
      "/home/hennes/.local/lib/python3.8/site-packages/dateparser/date_parser.py:35: PytzUsageWarning: The localize method is no longer necessary, as this time zone supports the fold attribute (PEP 495). For more details on migrating to a PEP 495-compliant implementation, see https://pytz-deprecation-shim.readthedocs.io/en/latest/migration.html\n",
      "  date_obj = stz.localize(date_obj)\n",
      "<ipython-input-14-60a6f40a3a3d>:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df100.loc[:,'date'] = df100.loc[:,'date'].apply(lambda x: dateparser.parse(x, settings={'STRICT_PARSING': True}))\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "### Some preliminary stuff\n",
    "\n",
    "# unify outlet names\n",
    "df.loc[df[\"page\"].str.startswith('Canal13'), \"page\"] = \"Canal13\"\n",
    "df.loc[df[\"page\"].str.startswith('100% Noticias'), \"page\"] = \"100% Noticias\"\n",
    "df.loc[df[\"page\"].str.startswith('Confidencial'), \"page\"] = \"Confidencial\"\n",
    "df.loc[df[\"page\"].str.startswith('Radio Corporacion'), \"page\"] = \"Radio Corporacion\"\n",
    "df.loc[df[\"page\"].str.startswith('Canal10'), \"page\"] = \"Canal10\"\n",
    "df.loc[df[\"page\"].str.startswith('Canal14'), \"page\"] = \"Canal14\"\n",
    "\n",
    "\n",
    "\n",
    "# extract canal13 dates \n",
    "df.loc[df[\"page\"] == 'Canal13', [\"date\"]] = df.loc[df[\"page\"] == 'Canal13',\n",
    "                                                 \"url\"].str.extract(r'(\\d\\d\\d\\d/\\d\\d/\\d\\d)').values\n",
    "# extract canal 6 dates\n",
    "df.loc[df[\"page\"] == 'Canal6', [\"date\"]] = df.loc[df[\"page\"] == 'Canal6',\n",
    "\n",
    "                                                \"url\"].str.extract(r'(\\d\\d\\d\\d/\\d\\d/\\d\\d)').values\n",
    "# extract radio 800 dates\n",
    "df.loc[df[\"page\"] == 'Radio 800', [\"date\"]] = df.loc[df[\"page\"] == 'Radio 800',\n",
    "                                                   \"url\"].str.extract(r'(\\d\\d\\d\\d/\\d\\d/\\d\\d)').values\n",
    "\n",
    "# convert canal10 date to datetime\n",
    "df10 = df.loc[df[\"page\"] == \"Canal10\"]\n",
    "df10.loc[:,'date'] = df10['date'].str.replace('de ', '', regex=True)\n",
    "df10.loc[:,'date'] = df10['date'].str.replace(r'Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday', '', regex=True)\n",
    "df10.loc[:,'date'] = pd.to_datetime(df10['date'])\n",
    "df.loc[df[\"page\"] == \"Canal10\", 'date'] = df10[\"date\"].to_list()\n",
    "\n",
    "# convert canal14 date to datetime\n",
    "# first delete entries where only time of day was extracted\n",
    "df.loc[df[\"page\"] == \"Canal14\"] = df.loc[(df[\"page\"] == \"Canal14\") & (df[\"date\"].str.contains(r\"\\d{4}\", na=False))]\n",
    "# I am doing that with dateparser, because pandas cannot handle the spanish dates\n",
    "df14 = df.loc[df[\"page\"] == \"Canal14\"]\n",
    "df14.loc[:,'date'] = df14.loc[:,'date'].apply(lambda x: dateparser.parse(x, settings={'STRICT_PARSING': True}))\n",
    "df.loc[df[\"page\"] == \"Canal14\", 'date'] = df14[\"date\"].to_list()\n",
    "\n",
    "# 100 % noticias\n",
    "df100 = df.loc[df[\"page\"] == \"100% Noticias\"]\n",
    "# convert nan in date column to empty string so that dateparser works\n",
    "df100.fillna({\"date\":\" \"}, axis=0, inplace=True)\n",
    "df100.loc[:,'date'] = df100.loc[:,'date'].apply(lambda x: dateparser.parse(x, settings={'STRICT_PARSING': True}))\n",
    "df.loc[df[\"page\"] == \"100% Noticias\", 'date'] = df100[\"date\"].to_list()\n",
    "\n",
    "# canal2, canal4, confidencial, radio corporacion, radio nicaragua,\n",
    "# radio primerissima are already in datetime format\n",
    "\n",
    "# convert the rest to datetime\n",
    "df.loc[:,\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "# create year variable\n",
    "df[\"year\"] = pd.DatetimeIndex(df.date).year\n",
    "\n",
    "df.to_csv(\"dataset1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Tokenization Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles without text per outlet\n",
      "page\n",
      "100% Noticias              29\n",
      "Canal10                  1114\n",
      "Canal13                 10019\n",
      "Canal2                     31\n",
      "Canal4                      2\n",
      "Canal6                     11\n",
      "Radio Corporacion         119\n",
      "Radio Nicaragua           220\n",
      "Radio la Primerisima        7\n",
      "dtype: int64\n",
      "\n",
      "Number of articles without text per year\n",
      "year\n",
      "2011.0      72\n",
      "2012.0    1617\n",
      "2013.0    3437\n",
      "2014.0    1898\n",
      "2015.0    2084\n",
      "2016.0    1176\n",
      "2017.0     280\n",
      "2018.0     293\n",
      "2019.0     183\n",
      "2020.0     447\n",
      "2021.0      56\n",
      "2022.0       4\n",
      "dtype: int64\n",
      "year                  2011.0  2012.0  2013.0  2014.0  2015.0  2016.0  2017.0  \\\n",
      "page                                                                           \n",
      "100% Noticias            NaN     NaN     NaN     NaN  1743.0  3249.0   903.0   \n",
      "Canal10                  NaN     NaN     NaN     NaN     NaN  3631.0  3181.0   \n",
      "Canal13                168.0   552.0     6.0  1880.0  2182.0  4997.0  5859.0   \n",
      "Canal14                  NaN     NaN     NaN     NaN     NaN     NaN    59.0   \n",
      "Canal2                   NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "Canal4                   NaN     NaN     NaN     NaN     NaN     NaN   685.0   \n",
      "Canal6                   NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "Confidencial             NaN     NaN     NaN     NaN   385.0   785.0   632.0   \n",
      "Radio 800                NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "Radio Corporacion        NaN     NaN     NaN     NaN    62.0   490.0   499.0   \n",
      "Radio Nicaragua          NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "Radio la Primerisima     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "\n",
      "year                  2018.0  2019.0  2020.0   2021.0  2022.0  \n",
      "page                                                           \n",
      "100% Noticias         3072.0  1270.0  3825.0   3910.0   495.0  \n",
      "Canal10               1720.0  1585.0  6064.0   2848.0   766.0  \n",
      "Canal13               6811.0  5262.0  3923.0   3519.0   455.0  \n",
      "Canal14               1420.0  1264.0  1517.0   1380.0   581.0  \n",
      "Canal2                   NaN     NaN   674.0   1732.0   199.0  \n",
      "Canal4                6944.0  3270.0  1164.0   3162.0   413.0  \n",
      "Canal6                  34.0  1394.0  1138.0   2313.0   327.0  \n",
      "Confidencial          1190.0  1175.0  1724.0   1972.0   237.0  \n",
      "Radio 800              367.0   217.0   146.0     66.0     9.0  \n",
      "Radio Corporacion     1594.0  1723.0  1767.0   2610.0   319.0  \n",
      "Radio Nicaragua       1273.0  2286.0  9559.0   7240.0  1004.0  \n",
      "Radio la Primerisima     NaN     NaN  2307.0  10784.0  1873.0  \n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv(\"dataset1.csv\")\n",
    "\n",
    "# delete duplicate articles\n",
    "df.drop_duplicates(subset=\"url\", inplace=True, ignore_index=True)\n",
    "\n",
    "# check for articles that do not have any text in them\n",
    "print(\"Number of articles without text per outlet\")\n",
    "print(df[df.text.isna()].groupby('page').size())\n",
    "\n",
    "# there are quite some articles without text. I randomly selected some of these to verify that\n",
    "# this is not a mistake in my crawler, but that they just don't have text. Indeed, they are all\n",
    "# articles that only contain videos. This is reflected by the fact that the outlets in thise list\n",
    "# are mostly TV and radio stations.\n",
    "\n",
    "print(\"\\nNumber of articles without text per year\")\n",
    "print(df[df.text.isna()].groupby('year').size())\n",
    "\n",
    "# Because the models need text to work, I will discard rows without text\n",
    "\n",
    "df = df.loc[~df.text.isna()].reset_index(drop = True)\n",
    "\n",
    "# there is also a discrepancy in when the outlets started their work, as can be seen from the following table\n",
    "print(df.groupby(['year', \"page\"]).agg({\"text\":np.size}).reset_index().pivot(\"page\", \"year\", \"text\"))\n",
    "\n",
    "# Because before 2015, only a single government outlet uploaded articles, I will keep only articles published\n",
    "# from 2015 on\n",
    "\n",
    "df = df.loc[df[\"year\"]>2014]\n",
    "\n",
    "# removing boilerplate, news agency sources, hyperlinks etc.\n",
    "\n",
    "             # newlines, tabs etc.\n",
    "repl_dict = {r\"\\t|\\n|\\r|\\xa0\":\" \",\n",
    "             # whitespace\n",
    "             r\"\\s{2,}\":\" \"}\n",
    "df.replace({\"text\":repl_dict, \"title\":repl_dict}, regex=True, inplace = True)\n",
    "\n",
    "            # hashtags\n",
    "repl_dict = {\"#\": \"\",\n",
    "             # source in some articles\n",
    "            r'Fuente: El 19 Digital|Fuente: TN8':\"\",\n",
    "             # all hyperlinks\n",
    "            r\"http\\S+\":\"\",\n",
    "             # canal10 boilerplate\n",
    "            r\"Foto: Shutterstock.*\": \"\",\n",
    "            r\"Foto:.*{.*\": \"\",\n",
    "            \"Noticias de Nicaragua y el Mundo\": \"\",\n",
    "            r\"p(\\.\\w+\\s|\\s)\\{.*?\\}\": \"\",\n",
    "            r\"\\{.*?\\}\": \"\",\n",
    "            r\"Normal.*X-NONE( /\\* Style Definitions \\*/ table\\.MsoNormalTable)?\": \"\",\n",
    "             # canal4 boilerplate\n",
    "            r\"Canal 4 Noticias[\\s\\S]+Canal 4 Nicaragua. Todos los derechos reservados\": \"\",\n",
    "            r\"Comparte[.\\s]*?esto:[.\\s]*?Tweet[.\\s]*?WhatsApp[.\\s]*?Telegram\": \"\",\n",
    "            \"LEER TAMBIÉN\": \"\",\n",
    "            \"Leer más:\": \"\",\n",
    "            \"AMPLIACIÓN EN BREVE…\": \"\",\n",
    "            r\"Canal 4 Noticias de Nicaragua.*\": \"\",\n",
    "             \"Periodista en Multinoticias, Canal 4\": \"\",\n",
    "             # news agency\n",
    "            \"(EFE)\":\"\",\n",
    "             # source mentioned at end of article\n",
    "            r\"Con información de\\:+$\": \"\",\n",
    "             # copyright stuff\n",
    "            \"© 100% Noticias ¡Con primicias a toda hora!\":\"\",\n",
    "            \"© Getty Images\":\"\",\n",
    "            r\"©\\s?[Vv]iva [Nn]icaragua,? (Canal 13 )?(Previous Next)?\": \"\",\n",
    "            \"© AFP\":\"\",\n",
    "            \"© AP\":\"\",\n",
    "            \"© creative commons\": \"\",\n",
    "            \"© El 19 Digital\": \"\",\n",
    "            \"© Consejo de Comunicación y Ciudadanía\": \"\",\n",
    "            \"© Juventud Presidente\": \"\",\n",
    "            \"© Ministerio de Gobernación\": \"\",\n",
    "             # copyrights for photographers\n",
    "            r\"\\w+?\\s+?\\w+?\\s+?©\":\"\",\n",
    "             # article suggestions\n",
    "            \"Te recomendamos:\": \"\",\n",
    "            \"Quizás te interesa:\":\"\",\n",
    "            \"Lee Aquí:\": \"\",\n",
    "             # twitter links\n",
    "            r\"pic\\.twitter\\.com.+?\\d{4}\": \"\",\n",
    "            r\"—.+?\\(@.+?\\).+?\\d{4}\":\"\",\n",
    "             # source information\n",
    "            r\"Con información de:.+$\": \"\",\n",
    "            # emails\n",
    "            r\"\\[email protected\\]\":\"\"}\n",
    "\n",
    "df.replace({\"text\":repl_dict, \"title\":repl_dict}, regex=True, inplace = True)\n",
    "\n",
    "# the same goes for articles with very short texts.\n",
    "# again, due to the focus on videos, some articles in the dataset consist only\n",
    "# of a single, short sentence. Articles with less than 100-character long texts will\n",
    "# therefore likewise be deleted.\n",
    "\n",
    "df = df.loc[df[\"text\"].apply(lambda x: len(str(x)) > 200)].reset_index(drop = True)\n",
    "\n",
    "df.to_csv(\"dataset_token_ready.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number of articles per outlet\n",
    "df.groupby('page').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save only text as csv file\n",
    "df = pd.read_csv(\"dataset_token_ready.csv\")[\"text\"]\n",
    "\n",
    "df.to_csv(\"texts.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text as generator\n",
    "csv_gen = (row for row in open(\"texts.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text as list\n",
    "textlist = pd.read_csv(\"dataset_token_ready.csv\")[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load spanish language model\n",
    "nlp = spacy.load('es_core_news_md', disable=[\"parser\", \"attribute_ruler\", \"ner\"])\n",
    "# add stopwords\n",
    "nlp.Defaults.stop_words |= {\"a\",\"y\", \"o\"}\n",
    "# reload language model to incorporate new stopwords \n",
    "nlp = spacy.load('es_core_news_md', disable=[\"parser\", \"attribute_ruler\", \"ner\"])\n",
    "\n",
    "tokenlist = []\n",
    "# go through rows of article texts\n",
    "for i, doc in enumerate(nlp.pipe(csv_gen, disable=[\"parser\", \"attribute_ruler\", \"ner\"], n_process=4)):\n",
    "    # append lists of lemmatised tokens to tokenlist\n",
    "    tokenlist.append([token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha])\n",
    "    if i % 10000 == 0 and i != 0: print(i)\n",
    "# remove first entry, which is the column title\n",
    "tokenlist = tokenlist[1:]\n",
    "# save to pickle\n",
    "with open(f\"tokens/tokens.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenlist, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# integrate tokenlist into df, useful for later structural topic model and all subsampling\n",
    "# load tokenlist\n",
    "with open(\"tokens/tokens.pkl\", \"rb\") as f:\n",
    "    tokenlist = pickle.load(f)\n",
    "\n",
    "# integrate tokenlist back into dataframe\n",
    "df = pd.read_csv(\"dataset_token_ready.csv\")\n",
    "df = df.assign(tokens=tokenlist)\n",
    "\n",
    "# save dataset as pickle (csv does not work, because it cannot save the lists of tokens as cell entries)\n",
    "df.to_pickle(\"dataset_with_tokens.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
