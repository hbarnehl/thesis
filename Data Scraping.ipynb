{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code necessary to scrape Nicaraguan news outlets' websites for the names, links, and textual contents of all of their news articles. The process proceeds in two steps. Firstly, article links and titles are retrieved. Secondly, the links thus gathered are used to scrape the textual content of the articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "from random import randint\n",
    "from time import sleep\n",
    "from time import time\n",
    "from itertools import cycle\n",
    "import pickle\n",
    "import concurrent\n",
    "import logging\n",
    "import glob\n",
    "from newsplease import NewsPlease\n",
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Titles and Links from the mainpages of the outlets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following dictionary, I collect the information necessary to loop over the news pages. Each entry specifies the name of the news outlet, the url, the number of pages, the xpath to access article titles and the xpath to access the article links. In some cases, the links only contain a part of the link. In those cases, the necessary prefix is included as a key too. In some cases, there are two xpaths which need to be accessed to retrieve all relevant titles and links. In those cases, title and link paths are recorded in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [{'name':'Canal2',\n",
    "        'url':'https://canal2tv.com/category/nacionales/page/',\n",
    "        'pages':132,\n",
    "        'titlepath': \"//div[@class='post-container']//a[@class='post-title']/h2\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//div[@class='post-container']//a[@class='post-title']\"},\n",
    "        {'name':'Canal4',\n",
    "        'url':'https://www.canal4.com.ni/nicaragua/page/',\n",
    "        'pages':1565,\n",
    "        'titlepath': \"//div[@class='tg-col-control']//h3/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//div[@class='tg-col-control']//h3/a\"},\n",
    "        {'name':'Canal6',\n",
    "        'url':'https://canal6.com.ni/category/nacionales/page/',\n",
    "        'pages':307,\n",
    "        'titlepath': \"//figure[@class='figure']//a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//figure[@class='figure']//a\"},\n",
    "        {'name':'Canal10',\n",
    "        'url':'https://www.canal10.com.ni/category/nacionales/page/',\n",
    "        'pages':1234,\n",
    "        'titlepath': \"//div[@class='item card-type-a child']//h2/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//div[@class='item card-type-a child']//h2/a\"},\n",
    "        {'name':'Canal13_politica',\n",
    "        'url':'https://www.vivanicaragua.com.ni/category/politica/page/',\n",
    "        'pages':445,\n",
    "        'titlepath': \"//a[@class='card-title']//h3\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//a[@class='card-title']\"},\n",
    "        {'name':'Canal13_economia',\n",
    "        'url':'https://www.vivanicaragua.com.ni/category/economia/page/',\n",
    "        'pages':363,\n",
    "        'titlepath': \"//a[@class='card-title']//h3\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//a[@class='card-title']\"},\n",
    "        {'name':'Canal13_sociales',\n",
    "        'url':'https://www.vivanicaragua.com.ni/category/sociales/page/',\n",
    "        'pages':2997,\n",
    "        'titlepath': \"//a[@class='card-title']//h3\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//a[@class='card-title']\"},\n",
    "        {'name':'Canal14',\n",
    "        'url':'https://www.vostv.com.ni/nacionales/?page=',\n",
    "        'pages':669,\n",
    "        'titlepath': \"//section[@class='secondary-news']//h3\",\n",
    "        'prefix': 'https://www.vostv.com.ni',\n",
    "        'linkpath': \"//section[@class='secondary-news']//div[@class='figure-cap']/a[1]\"},\n",
    "        {'name':'Radio la Primerisima',\n",
    "        'url':'https://radiolaprimerisima.com/noticias-generales/page/',\n",
    "        'pages':797,\n",
    "        'titlepath': \"//div[@class='post_title']//a/span[1]\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//div[@class='post_title']//a\"},\n",
    "        {'name':'La Nueva Radio Ya',\n",
    "        'url':'https://nuevaya.com.ni/nacionales/page/',\n",
    "        'pages':1430,\n",
    "        'titlepath': \"//div[@class='vc_column tdi_52 wpb_column vc_column_container tdc-column td-pb-span9']//h3[@class='entry-title td-module-title']//a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//div[@class='vc_column tdi_52 wpb_column vc_column_container tdc-column td-pb-span9']//h3[@class='entry-title td-module-title']//a\"},\n",
    "        {'name':'Radio 800',\n",
    "        'url':'https://radio800ni.com/category/nacionales/page/',\n",
    "        'pages':81,\n",
    "        'titlepath': \"//h2[@class='post-title']/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//h2[@class='post-title']/a\"},\n",
    "        {'name':'Radio Nicaragua',\n",
    "        'url':'https://radionicaragua.com.ni/category/nacionales/page/',\n",
    "        'pages':2161,\n",
    "        'titlepath': \"//figcaption/a/h2\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//figcaption/a\"},\n",
    "        {'name':'Radio Corporacion_nacional',\n",
    "        'url':'https://radio-corporacion.com/blog/archivos/category/nacional/page/',\n",
    "        'pages':584,\n",
    "        'titlepath': \"//h3[@class='mh-loop-title']/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//h3[@class='mh-loop-title']/a\"},\n",
    "        {'name':'Radio Corporacion_politica',\n",
    "        'url':'https://radio-corporacion.com/blog/archivos/category/politica/page/',\n",
    "        'pages':264,\n",
    "        'titlepath': \"//h3[@class='mh-loop-title']/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//h3[@class='mh-loop-title']/a\"},\n",
    "        {'name':'Radio Corporacion_eco',\n",
    "        'url':'https://radio-corporacion.com/blog/archivos/category/eco/page/',\n",
    "        'pages':116,\n",
    "        'titlepath': \"//h3[@class='mh-loop-title']/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//h3[@class='mh-loop-title']/a\"},\n",
    "        {'name':'Confidencial_politica',\n",
    "        'url':'https://www.confidencial.com.ni/politica/page/',\n",
    "        'pages':355,\n",
    "        'titlepath': \"//h2[@class='archive-titles']/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//h2[@class='archive-titles']/a\"},\n",
    "        {'name':'Confidencial_economia',\n",
    "        'url':'https://www.confidencial.com.ni/economia/page/',\n",
    "        'pages':168,\n",
    "        'titlepath': \"//h2[@class='archive-titles']/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//h2[@class='archive-titles']/a\"},\n",
    "        {'name':'Confidencial_nacion',\n",
    "        'url':'https://www.confidencial.com.ni/nacion/page/',\n",
    "        'pages':637,\n",
    "        'titlepath': \"//h2[@class='archive-titles']/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//h2[@class='archive-titles']/a\"},\n",
    "        {'name':'100% Noticias_nacionales',\n",
    "        'url':'https://100noticias.com.ni/nacionales/?page=',\n",
    "        'pages':747,\n",
    "        'titlepath': [\"//div[@class='col-md-6 m-bottom-10']//a//h5\", \"//div[@class='col-6 col-md-4']/a//h5\"],\n",
    "        'prefix' : \"https://100noticias.com.ni\",\n",
    "        'linkpath': [\"//div[@class='col-md-6 m-bottom-10']//a\", \"//div[@class='col-6 col-md-4']/a\"]},\n",
    "        {'name':'100% Noticias_economia',\n",
    "        'url':'https://100noticias.com.ni/economia/?page=',\n",
    "        'pages':73,\n",
    "        'titlepath': [\"//div[@class='col-md-6 m-bottom-10']//a//h5\", \"//div[@class='col-6 col-md-4']/a//h5\"],\n",
    "        'prefix' : \"https://100noticias.com.ni\",\n",
    "        'linkpath': [\"//div[@class='col-md-6 m-bottom-10']//a\", \"//div[@class='col-6 col-md-4']/a\"]},\n",
    "        {'name':'100% Noticias_politica',\n",
    "        'url':'https://100noticias.com.ni/politica/?page=',\n",
    "        'pages':114,\n",
    "        'titlepath': [\"//div[@class='col-md-6 m-bottom-10']//a//h5\", \"//div[@class='col-6 col-md-4']/a//h5\"],\n",
    "        'prefix': \"https://100noticias.com.ni\",\n",
    "        'linkpath': [\"//div[@class='col-md-6 m-bottom-10']//a\", \"//div[@class='col-6 col-md-4']/a\"]}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function executes the scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles(outlet):\n",
    "    \n",
    "    headers = requests.utils.default_headers()\n",
    "    headers['User-Agent'] = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'\n",
    "    linklist = []\n",
    "    titlelist = []\n",
    "    baseurl = outlet['url']\n",
    "    name = outlet['name']\n",
    "    \n",
    "    logger.info(f'Working on {name}.')\n",
    "    \n",
    "    # start looping through pages\n",
    "    for i in range(1, outlet['pages']+1):\n",
    "        # report on status at every ten pages\n",
    "        if i % 10 == 0: logger.info(f\"Status {name}: page {i}\")\n",
    "        try:\n",
    "            url = baseurl+str(i)\n",
    "            source = requests.get(url, headers=headers).text\n",
    "            tree = html.fromstring(source)\n",
    "            if isinstance(outlet['titlepath'] , list):\n",
    "                links = [outlet['prefix'] + l.attrib['href'] for l in (tree.xpath(outlet[\"linkpath\"][0]) + tree.xpath(outlet[\"linkpath\"][1]))]\n",
    "                titles = [l.text for l in (tree.xpath(outlet[\"titlepath\"][0]) + tree.xpath(outlet[\"titlepath\"][1]))]\n",
    "            else:\n",
    "                links = [outlet['prefix'] + l.attrib['href']for l in tree.xpath(outlet[\"linkpath\"])]\n",
    "                titles = [l.text for l in tree.xpath(outlet[\"titlepath\"])]\n",
    "            [linklist.append(x) for x in links]\n",
    "            [titlelist.append(x) for x in titles]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error with {name} at page {i}:\")\n",
    "            logger.error(e)\n",
    "        sleep(randint(3, 6))\n",
    "    \n",
    "    combined = [linklist, titlelist]\n",
    "    with open(f'{name}-links-titles.pkl', 'wb') as f:\n",
    "        pickle.dump(combined, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creating and Configuring Logger\n",
    "Log_Format = \"%(levelname)s %(asctime)s - %(message)s\"\n",
    "\n",
    "logging.basicConfig(filename = f\"logfile.log\",\n",
    "                    filemode = \"w\",\n",
    "                    format = Log_Format, \n",
    "                    level = logging.INFO,\n",
    "                    force = True)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    executor.map(scrape_articles, [pages[8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merge individual files into df and save to csv\n",
    "df = pd.DataFrame(columns = ['page', 'date', 'title', 'text', 'link'])\n",
    "for file in [x for x in glob.glob(\"*\") if x.endswith(\".pkl\")]:\n",
    "    with open(file, \"rb\") as f:\n",
    "        articles = pickle.load(f)\n",
    "    name = file.split(\"-\")[0]\n",
    "    df = df.append(pd.DataFrame({'link': articles[0], 'title': articles[1]}))\n",
    "    df.page.fillna(value = name, inplace =True)\n",
    "df.to_csv('data.csv', sep= \";\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unifying individual categories in to single pickle files\n",
    "with open(\"Radio Corporacion_eco-links-titles.pkl\", \"rb\") as f:\n",
    "    articles1 = pickle.load(f)\n",
    "with open(\"Radio Corporacion_nacional-links-titles.pkl\", \"rb\") as f:\n",
    "    articles2 = pickle.load(f)\n",
    "with open(\"Radio Corporacion_politica-links-titles.pkl\", \"rb\") as f:\n",
    "    articles3 = pickle.load(f)\n",
    "    \n",
    "articles[0] = articles1[0] + articles2[0] + articles3[0]\n",
    "articles[1] = articles1[1] + articles2[1] + articles3[1]\n",
    "\n",
    "with open(f'Radio Corporacion.pkl', 'wb') as f:\n",
    "        pickle.dump(articles, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using newsplease to scrape article text and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one goes through articles per outlet. It is designed to be used with multithreading (one thread per outlet)\n",
    "outlets_instructions = [{'name': 'Canal2-links-titles.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': 'Canal6-links-titles.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': 'Canal10-links-titles.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': 'Canal13-links-titles.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': 'Confidencial-links-titles.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': 'Radio Corporacion-links-titles.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': 'Radio la Primerisima-links-titles.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': 'Radio Nicaragua-links-titles.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': '100% Noticias-links-titles.pkl',\n",
    "                         'xpath': \"//div[@class='story-body']\"},\n",
    "                        {'name': 'Radio 800-links-titles.pkl',\n",
    "                         'xpath': \"//div[@class='post-body padd-top']/p\"},\n",
    "                        {'name': 'Canal14-links-titles.pkl',\n",
    "                         'xpath': \"//div[@class='story-body']\"},\n",
    "                        {'name': 'Canal4-links-titles.pkl',\n",
    "                         'xpath': \"//span[@style='color: #000000;']\"}]\n",
    "\n",
    "# made a copy of other instructions, because I need to run scraper again, this time only looking for date\n",
    "outlets_instructions1 = [{'name': 'Canal2-links-titles.pkl_full.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': 'Canal10-links-titles.pkl_full.pkl',\n",
    "                         'xpath': \"//div[@class='date']\"},\n",
    "                        {'name': 'Confidencial-links-titles.pkl_full.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': 'Radio Corporacion-links-titles.pkl_full.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': 'Radio la Primerisima-links-titles.pkl_full.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': 'Radio Nicaragua-links-titles.pkl_full.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': '100% Noticias-links-titles.pkl_full.pkl',\n",
    "                         'xpath': \"//div[@class='story-meta top-meta text-center']/span[2]\"},\n",
    "                        {'name': 'Canal14-links-titles.pkl_full.pkl',\n",
    "                         'xpath': \"//ul[@class='story-meta m-bottom-20']/li[2]\"},\n",
    "                        {'name': 'Canal4-links-titles.pkl_full.pkl',\n",
    "                         'xpath': None}]\n",
    "\n",
    "def scrape_text(outlet):\n",
    "    headers = requests.utils.default_headers()\n",
    "    headers['User-Agent'] = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'\n",
    "    text_list = []\n",
    "    with open(outlet[\"name\"], \"rb\") as f:\n",
    "        articles = pickle.load(f)\n",
    "    logger.info(f'Working on {outlet[\"name\"]}.')\n",
    "    if outlet['xpath'] is None:\n",
    "        for url in articles[0]:\n",
    "            try:\n",
    "                art = NewsPlease.from_url(url)\n",
    "                text = art.maintext\n",
    "                text_list.append(text)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"{outlet['name']}-{url}: {e}\")\n",
    "                text_list.append(None)\n",
    "            sleep(randint(3, 6))\n",
    "    else:\n",
    "        for url in articles[0]:\n",
    "            try:\n",
    "                source = requests.get(url, headers=headers).text\n",
    "                tree = html.fromstring(source)\n",
    "                text = \" \".join([l.text_content() for l in tree.xpath(outlet['xpath'])])\n",
    "                text_list.append(text)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"{outlet['name']}-{url}: {e}\")\n",
    "                text_list.append(None)\n",
    "            sleep(randint(3, 6))\n",
    "    combined = [articles[0], articles[1], text_list]\n",
    "    with open(f'{outlet[\"name\"]}_full.pkl', 'wb') as f:\n",
    "        pickle.dump(combined, f)\n",
    "\n",
    "def scrape_date(outlet):\n",
    "    headers = requests.utils.default_headers()\n",
    "    headers['User-Agent'] = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'\n",
    "    date_list = []\n",
    "    with open(outlet[\"name\"], \"rb\") as f:\n",
    "        articles = pickle.load(f)\n",
    "    logger.warning(f'Working on {outlet[\"name\"]}.')\n",
    "    if outlet['xpath'] is None:\n",
    "        for url in articles[0]:\n",
    "            logger.warning(f'{outlet[\"name\"]}: {url}.')\n",
    "            try:\n",
    "                art = NewsPlease.from_url(url)\n",
    "                date = art.date_publish\n",
    "                date_list.append(date)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"{outlet['name']}-{url}: {e}\")\n",
    "                date_list.append(None)\n",
    "            sleep(randint(3, 6))\n",
    "    else:\n",
    "        for url in articles[0]:\n",
    "            logger.warning(f'{outlet[\"name\"]}: {url}.')\n",
    "            try:\n",
    "                source = requests.get(url, headers=headers).text\n",
    "                tree = html.fromstring(source)\n",
    "                date = \" \".join([l.text_content() for l in tree.xpath(outlet['xpath'])])\n",
    "                date_list.append(date)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"{outlet['name']}-{url}: {e}\")\n",
    "                date_list.append(None)\n",
    "            sleep(randint(3, 6))\n",
    "    combined = [articles[0], articles[1], articles[2], date_list]\n",
    "    with open(f'{outlet[\"name\"]}_full.pkl', 'wb') as f:\n",
    "        pickle.dump(combined, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Scraping only for Canal13, which has so many articles I need to multithread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one was made for a single outlet.\n",
    "# Multithreading should be used here to go through a couple of urls at the same time.\n",
    "def scrape_13(page):\n",
    "    if 'text' not in canal13_list[page]:\n",
    "        if page % 50 == 0: logger.warning(f\"Status: url {page}\")\n",
    "        url = canal13_list[page]['url']\n",
    "        try:\n",
    "            art = NewsPlease.from_url(url)\n",
    "            text = art.maintext\n",
    "            canal13_list[page].update({\"text\": text})\n",
    "        except Exception as e:\n",
    "            text = None\n",
    "            logger.error(f\"{url}: {e}\")\n",
    "        sleep(randint(3, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming to list of dicts\n",
    "with open('Canal13-links-titles.pkl', \"rb\") as f:\n",
    "    articles = pickle.load(f)\n",
    "\n",
    "canal13_list = []\n",
    "for i in range(len(articles[0])):\n",
    "    canal13_list.append({\"url\":articles[0][i], \"title\": articles[1][i]})\n",
    "    \n",
    "with open(\"Canal13-dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(canal13_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will load the canal13 file into memory, work on urls without text and save once interrupted\n",
    "with open(\"Canal13-dict.pkl\", \"rb\") as f:\n",
    "    canal13_list = pickle.load(f)\n",
    "\n",
    "Log_Format = \"%(levelname)s %(asctime)s - %(message)s\"\n",
    "\n",
    "logging.basicConfig(filename = f\"log_canal13.log\",\n",
    "                    format = Log_Format, \n",
    "                    level = logging.WARNING,\n",
    "                    force = True)\n",
    "logger = logging.getLogger()\n",
    "try:\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        executor.map(scrape_13, range(len(canal13_list)))\n",
    "except KeyboardInterrupt:\n",
    "    for i in range(len(canal13_list)):\n",
    "        if 'text' in canal13_list[i]:\n",
    "            pass\n",
    "        else:\n",
    "            logger.warning(f\":Interrupted at {i}\")\n",
    "            break\n",
    "    with open(\"Canal13-dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(canal13_list, f)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41765\n",
      "41764\n"
     ]
    }
   ],
   "source": [
    "# checking which articles are still missing the text\n",
    "with open(\"Canal13-dict.pkl\", \"rb\") as f:\n",
    "    canal13_list = pickle.load(f)\n",
    "for i in reversed(range(len(canal13_list))):\n",
    "    if 'text' in canal13_list[i]:\n",
    "        pass\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on Canal2-links-titles.pkl_full.pkl\n",
      "work on https://canal2tv.com/nacionales/covid-19-minsa-recuperacion-nicaraguenses/\n",
      "work on https://canal2tv.com/nacionales/murillo-dios-ruben-sandino-gesta/\n",
      "work on https://canal2tv.com/nacionales/ortega-razonen-hecatombe-planeta/\n",
      "work on https://canal2tv.com/nacionales/estudiantes-secundaria-campo-recibiran-becas/\n",
      "work on https://canal2tv.com/nacionales/lluvias-provocadas-frente-frio/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-b11ed1ab01be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutlets_instructions1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mscrape_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-53-0d0530c83efb>\u001b[0m in \u001b[0;36mscrape_date\u001b[0;34m(outlet)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"work on {url}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                 \u001b[0mart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNewsPlease\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m                 \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate_publish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mdate_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/newsplease/__init__.py\u001b[0m in \u001b[0;36mfrom_url\u001b[0;34m(url, timeout)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNewsArticle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0marticles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNewsPlease\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marticles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marticles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/newsplease/__init__.py\u001b[0m in \u001b[0;36mfrom_urls\u001b[0;34m(urls, timeout)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleCrawler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNewsPlease\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleCrawler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/newsplease/__init__.py\u001b[0m in \u001b[0;36mfrom_html\u001b[0;34m(html, url, download_date, fetch_images)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'download_date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'modified_date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mtmp_article\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExtractedInformationStorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_relevant_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/newsplease/pipeline/extractor/article_extractor.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mextractor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractor_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0marticle_candidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0marticle_candidates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_candidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/newsplease/pipeline/extractor/extractors/abstract_extractor.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0marticle_candidate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_topimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0marticle_candidate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_author\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0marticle_candidate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublish_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0marticle_candidate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_language\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/newsplease/pipeline/extractor/extractors/date_extractor.py\u001b[0m in \u001b[0;36m_publish_date\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lxml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mpublish_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m                 \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m_feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/bs4/builder/_lxml.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParserError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._FeedParser.feed\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._FeedParser.feed\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._TargetParserContext._handleParseResult\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._TargetParserContext._handleParseResult\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/etree.pyx\u001b[0m in \u001b[0;36mlxml.etree._ExceptionContext._raise_if_stored\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/saxparser.pxi\u001b[0m in \u001b[0;36mlxml.etree._handleSaxEndNoNs\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._PythonSaxParserTarget._handleSaxEnd\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/bs4/builder/_lxml.py\u001b[0m in \u001b[0;36mend\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0mcompleted_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagStack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getNsTag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36mendData\u001b[0;34m(self, containerClass)\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0mcontainerClass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_container\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainerClass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontainerClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_was_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobject_was_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmost_recent_element\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36mobject_was_parsed\u001b[0;34m(self, o, parent, most_recent_element)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;31m# Check if we are inserting into an already parsed node.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_linkage_fixer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_linkage_fixer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m_linkage_fixer\u001b[0;34m(self, el)\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0;34m\"\"\"Make sure linkage of this fragment is sound.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0mdescendant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# running scraper with logging file without multithreading\n",
    "\n",
    "Log_Format = \"%(levelname)s %(asctime)s - %(message)s\"\n",
    "\n",
    "logging.basicConfig(filename = f\"logfile.log\",\n",
    "                    filemode = \"w\",\n",
    "                    format = Log_Format, \n",
    "                    level = logging.WARNING,\n",
    "                    force = True)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "for x in outlets_instructions1:\n",
    "    scrape_date(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running scraper with logging file with multithreading\n",
    "\n",
    "Log_Format = \"%(levelname)s %(asctime)s - %(message)s\"\n",
    "\n",
    "logging.basicConfig(filename = f\"logfile.log\",\n",
    "                    filemode = \"w\",\n",
    "                    format = Log_Format, \n",
    "                    level = logging.WARNING,\n",
    "                    force = True)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=9) as executor:\n",
    "    executor.map(scrape_date, outlets_instructions1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out xpaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n              Tuesday 03 de August 2021            '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for trying stuff out\n",
    "url = \"https://www.canal10.com.ni/noticia/violencia-sexual-en-el-vinculo-matrimonial/\"\n",
    "source = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}).text\n",
    "tree = html.fromstring(source)\n",
    "text = \"\".join([l.text_content() for l in tree.xpath(\"//div[@class='date']\")])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 2, 21, 0, 0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://radionicaragua.com.ni/artistas-y-escritores-rinden-homenaje-a-sandino-siempre-mas-alla/\"\n",
    "art = NewsPlease.from_url(url)\n",
    "date = art.date_publish\n",
    "date"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
