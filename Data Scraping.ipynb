{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code necessary to scrape Nicaraguan news outlets' websites for the names, links, and textual contents of all of their news articles. The process proceeds in two steps. Firstly, article links and titles are retrieved. Secondly, the links thus gathered are used to scrape the textual content of the articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "from random import randint\n",
    "from time import sleep\n",
    "from time import time\n",
    "from itertools import cycle\n",
    "import pickle\n",
    "import concurrent\n",
    "import logging\n",
    "import glob\n",
    "from newsplease import NewsPlease\n",
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Titles and Links from the mainpages of the outlets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following dictionary, I collect the information necessary to loop over the news pages. Each entry specifies the name of the news outlet, the url, the number of pages, the xpath to access article titles and the xpath to access the article links. In some cases, the links only contain a part of the link. In those cases, the necessary prefix is included as a key too. In some cases, there are two xpaths which need to be accessed to retrieve all relevant titles and links. In those cases, title and link paths are recorded in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [{'name':'Canal2',\n",
    "        'url':'https://canal2tv.com/category/nacionales/page/',\n",
    "        'pages':132,\n",
    "        'titlepath': \"//div[@class='post-container']//a[@class='post-title']/h2\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//div[@class='post-container']//a[@class='post-title']\"},\n",
    "        {'name':'Canal4',\n",
    "        'url':'https://www.canal4.com.ni/nicaragua/page/',\n",
    "        'pages':1565,\n",
    "        'titlepath': \"//div[@class='tg-col-control']//h3/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//div[@class='tg-col-control']//h3/a\"},\n",
    "        {'name':'Canal6',\n",
    "        'url':'https://canal6.com.ni/category/nacionales/page/',\n",
    "        'pages':307,\n",
    "        'titlepath': \"//figure[@class='figure']//a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//figure[@class='figure']//a\"},\n",
    "        {'name':'Canal10',\n",
    "        'url':'https://www.canal10.com.ni/category/nacionales/page/',\n",
    "        'pages':1234,\n",
    "        'titlepath': \"//div[@class='item card-type-a child']//h2/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//div[@class='item card-type-a child']//h2/a\"},\n",
    "        {'name':'Canal13_politica',\n",
    "        'url':'https://www.vivanicaragua.com.ni/category/politica/page/',\n",
    "        'pages':445,\n",
    "        'titlepath': \"//a[@class='card-title']//h3\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//a[@class='card-title']\"},\n",
    "        {'name':'Canal13_economia',\n",
    "        'url':'https://www.vivanicaragua.com.ni/category/economia/page/',\n",
    "        'pages':363,\n",
    "        'titlepath': \"//a[@class='card-title']//h3\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//a[@class='card-title']\"},\n",
    "        {'name':'Canal13_sociales',\n",
    "        'url':'https://www.vivanicaragua.com.ni/category/sociales/page/',\n",
    "        'pages':2997,\n",
    "        'titlepath': \"//a[@class='card-title']//h3\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//a[@class='card-title']\"},\n",
    "        {'name':'Canal14',\n",
    "        'url':'https://www.vostv.com.ni/nacionales/?page=',\n",
    "        'pages':669,\n",
    "        'titlepath': \"//section[@class='secondary-news']//h3\",\n",
    "        'prefix': 'https://www.vostv.com.ni',\n",
    "        'linkpath': \"//section[@class='secondary-news']//div[@class='figure-cap']/a[1]\"},\n",
    "        {'name':'Radio la Primerisima',\n",
    "        'url':'https://radiolaprimerisima.com/noticias-generales/page/',\n",
    "        'pages':797,\n",
    "        'titlepath': \"//div[@class='post_title']//a/span[1]\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//div[@class='post_title']//a\"},\n",
    "        {'name':'La Nueva Radio Ya',\n",
    "        'url':'https://nuevaya.com.ni/nacionales/page/',\n",
    "        'pages':1430,\n",
    "        'titlepath': \"//div[@class='vc_column tdi_52 wpb_column vc_column_container tdc-column td-pb-span9']//h3[@class='entry-title td-module-title']//a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//div[@class='vc_column tdi_52 wpb_column vc_column_container tdc-column td-pb-span9']//h3[@class='entry-title td-module-title']//a\"},\n",
    "        {'name':'Radio 800',\n",
    "        'url':'https://radio800ni.com/category/nacionales/page/',\n",
    "        'pages':81,\n",
    "        'titlepath': \"//h2[@class='post-title']/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//h2[@class='post-title']/a\"},\n",
    "        {'name':'Radio Nicaragua',\n",
    "        'url':'https://radionicaragua.com.ni/category/nacionales/page/',\n",
    "        'pages':2161,\n",
    "        'titlepath': \"//figcaption/a/h2\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//figcaption/a\"},\n",
    "        {'name':'Radio Corporacion_nacional',\n",
    "        'url':'https://radio-corporacion.com/blog/archivos/category/nacional/page/',\n",
    "        'pages':584,\n",
    "        'titlepath': \"//h3[@class='mh-loop-title']/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//h3[@class='mh-loop-title']/a\"},\n",
    "        {'name':'Radio Corporacion_politica',\n",
    "        'url':'https://radio-corporacion.com/blog/archivos/category/politica/page/',\n",
    "        'pages':264,\n",
    "        'titlepath': \"//h3[@class='mh-loop-title']/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//h3[@class='mh-loop-title']/a\"},\n",
    "        {'name':'Radio Corporacion_eco',\n",
    "        'url':'https://radio-corporacion.com/blog/archivos/category/eco/page/',\n",
    "        'pages':116,\n",
    "        'titlepath': \"//h3[@class='mh-loop-title']/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//h3[@class='mh-loop-title']/a\"},\n",
    "        {'name':'Confidencial_politica',\n",
    "        'url':'https://www.confidencial.com.ni/politica/page/',\n",
    "        'pages':355,\n",
    "        'titlepath': \"//h2[@class='archive-titles']/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//h2[@class='archive-titles']/a\"},\n",
    "        {'name':'Confidencial_economia',\n",
    "        'url':'https://www.confidencial.com.ni/economia/page/',\n",
    "        'pages':168,\n",
    "        'titlepath': \"//h2[@class='archive-titles']/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//h2[@class='archive-titles']/a\"},\n",
    "        {'name':'Confidencial_nacion',\n",
    "        'url':'https://www.confidencial.com.ni/nacion/page/',\n",
    "        'pages':637,\n",
    "        'titlepath': \"//h2[@class='archive-titles']/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//h2[@class='archive-titles']/a\"},\n",
    "        {'name':'100% Noticias_nacionales',\n",
    "        'url':'https://100noticias.com.ni/nacionales/?page=',\n",
    "        'pages':747,\n",
    "        'titlepath': [\"//div[@class='col-md-6 m-bottom-10']//a//h5\", \"//div[@class='col-6 col-md-4']/a//h5\"],\n",
    "        'prefix' : \"https://100noticias.com.ni\",\n",
    "        'linkpath': [\"//div[@class='col-md-6 m-bottom-10']//a\", \"//div[@class='col-6 col-md-4']/a\"]},\n",
    "        {'name':'100% Noticias_economia',\n",
    "        'url':'https://100noticias.com.ni/economia/?page=',\n",
    "        'pages':73,\n",
    "        'titlepath': [\"//div[@class='col-md-6 m-bottom-10']//a//h5\", \"//div[@class='col-6 col-md-4']/a//h5\"],\n",
    "        'prefix' : \"https://100noticias.com.ni\",\n",
    "        'linkpath': [\"//div[@class='col-md-6 m-bottom-10']//a\", \"//div[@class='col-6 col-md-4']/a\"]},\n",
    "        {'name':'100% Noticias_politica',\n",
    "        'url':'https://100noticias.com.ni/politica/?page=',\n",
    "        'pages':114,\n",
    "        'titlepath': [\"//div[@class='col-md-6 m-bottom-10']//a//h5\", \"//div[@class='col-6 col-md-4']/a//h5\"],\n",
    "        'prefix': \"https://100noticias.com.ni\",\n",
    "        'linkpath': [\"//div[@class='col-md-6 m-bottom-10']//a\", \"//div[@class='col-6 col-md-4']/a\"]}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function executes the scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is only to scrape the additional articles of Canal10\n",
    "\n",
    "pages = {'name':'Canal10_Accion_10',\n",
    "        'url':'https://www.canal10.com.ni/category/accion-10/page/',\n",
    "        'pages':4014,\n",
    "        'titlepath': \"//div[@class='item card-type-a child']//h2/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//div[@class='item card-type-a child']//h2/a\"}\n",
    "\n",
    "outlets_instructions = {'name': 'Canal10-Accion-links-titles.pkl',\n",
    "                         'xpath': None,\n",
    "                         'date': \"//div[@class='date']\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_10(page):\n",
    "    headers = requests.utils.default_headers()\n",
    "    headers['User-Agent'] = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'\n",
    "    if page % 50 == 0: logger.warning(f\"Status: url {page}\")\n",
    "    url = canal10_list[page]['url']\n",
    "    try:\n",
    "        source = requests.get(url, headers=headers).text\n",
    "        tree = html.fromstring(source)\n",
    "        art = NewsPlease.from_html(source)\n",
    "        text = art.maintext\n",
    "        date = \" \".join([l.text_content() for l in tree.xpath(outlets_instructions['date'])])\n",
    "        canal10_list[page].update({\"text\": text, \"date\": date})\n",
    "        with open(f'Canal10-accion.pkl', 'wb') as f:\n",
    "            pickle.dump(canal10_list, f)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"{outlets_instructions['name']}-{url}: {e}\")\n",
    "        text_list.append(None)\n",
    "    sleep(randint(1, 3))\n",
    "\n",
    "\n",
    "def scrape_art_10(i):\n",
    "    headers = requests.utils.default_headers()\n",
    "    headers['User-Agent'] = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'\n",
    "    if i % 50 == 0: logger.warning(f\"Status: url {i}\")\n",
    "    url = pages['url']+str(i)\n",
    "    name = pages['name']\n",
    "    \n",
    "    logger.info(f'Working on {name}.')\n",
    "    \n",
    "    # start looping through pages\n",
    "    try:\n",
    "        source = requests.get(url, headers=headers).text\n",
    "        tree = html.fromstring(source)\n",
    "        links = [pages['prefix'] + l.attrib['href']for l in tree.xpath(pages[\"linkpath\"])]\n",
    "        titles = [l.text for l in tree.xpath(pages[\"titlepath\"])]\n",
    "        [linklist.append(x) for x in links]\n",
    "        [titlelist.append(x) for x in titles]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error with {name} at page {i}:\")\n",
    "        logger.error(e)\n",
    "    sleep(randint(1, 3))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the links and article scraper\n",
    "Log_Format = \"%(levelname)s %(asctime)s - %(message)s\"\n",
    "\n",
    "logging.basicConfig(filename = f\"log_canal10.log\",\n",
    "                    format = Log_Format, \n",
    "                    level = logging.WARNING,\n",
    "                    force = True)\n",
    "logger = logging.getLogger()\n",
    "try:\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        executor.map(scrape_art_10, range(pages[\"pages\"]))\n",
    "except KeyboardInterrupt:\n",
    "    print(\"hallo\")\n",
    "    \n",
    "combined = [linklist, titlelist]\n",
    "with open(f\"Canal10_Accion_10-links-titles.pkl\", 'wb') as f:\n",
    "    pickle.dump(combined, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"Canal10_Accion_10-links-titles.pkl\", 'rb') as f:\n",
    "    canal10_list = pickle.load(f)\n",
    "c10 = []\n",
    "for i in range(len(canal10_list[1])):\n",
    "    c10.append({\"url\": canal10_list[0][i], \"title\":canal10_list[1][i]})\n",
    "canal10_list = c10\n",
    "canal10_list = canal10_list[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the text and date scraper\n",
    "    \n",
    "Log_Format = \"%(levelname)s %(asctime)s - %(message)s\"\n",
    "\n",
    "logging.basicConfig(filename = f\"log_canal10.log\",\n",
    "                    format = Log_Format, \n",
    "                    level = logging.WARNING,\n",
    "                    force = True)\n",
    "logger = logging.getLogger()\n",
    "try:\n",
    "    for x in range(len(canal10_list)):\n",
    "        scrape_10(x)\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        executor.map(scrape_10, range(len(canal10_list)))\n",
    "except KeyboardInterrupt:\n",
    "    for i in range(len(canal10_list)):\n",
    "        if 'text' in canal10_list[i]:\n",
    "            pass\n",
    "        else:\n",
    "            logger.warning(f\":Interrupted at {i}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles(outlet):\n",
    "    \n",
    "    headers = requests.utils.default_headers()\n",
    "    headers['User-Agent'] = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'\n",
    "    linklist = []\n",
    "    titlelist = []\n",
    "    baseurl = outlet['url']\n",
    "    name = outlet['name']\n",
    "    \n",
    "    logger.info(f'Working on {name}.')\n",
    "    \n",
    "    # start looping through pages\n",
    "    for i in range(1, outlet['pages']+1):\n",
    "        # report on status at every ten pages\n",
    "        if i % 10 == 0: logger.info(f\"Status {name}: page {i}\")\n",
    "        try:\n",
    "            url = baseurl+str(i)\n",
    "            source = requests.get(url, headers=headers).text\n",
    "            tree = html.fromstring(source)\n",
    "            if isinstance(outlet['titlepath'] , list):\n",
    "                links = [outlet['prefix'] + l.attrib['href'] for l in (tree.xpath(outlet[\"linkpath\"][0]) + tree.xpath(outlet[\"linkpath\"][1]))]\n",
    "                titles = [l.text for l in (tree.xpath(outlet[\"titlepath\"][0]) + tree.xpath(outlet[\"titlepath\"][1]))]\n",
    "            else:\n",
    "                links = [outlet['prefix'] + l.attrib['href']for l in tree.xpath(outlet[\"linkpath\"])]\n",
    "                titles = [l.text for l in tree.xpath(outlet[\"titlepath\"])]\n",
    "            [linklist.append(x) for x in links]\n",
    "            [titlelist.append(x) for x in titles]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error with {name} at page {i}:\")\n",
    "            logger.error(e)\n",
    "        sleep(randint(3, 6))\n",
    "    \n",
    "    combined = [linklist, titlelist]\n",
    "    with open(f'{name}-links-titles.pkl', 'wb') as f:\n",
    "        pickle.dump(combined, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creating and Configuring Logger\n",
    "Log_Format = \"%(levelname)s %(asctime)s - %(message)s\"\n",
    "\n",
    "logging.basicConfig(filename = f\"logfile.log\",\n",
    "                    filemode = \"w\",\n",
    "                    format = Log_Format, \n",
    "                    level = logging.INFO,\n",
    "                    force = True)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    executor.map(scrape_articles, [pages[8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merge individual files into df and save to csv\n",
    "df = pd.DataFrame(columns = ['page', 'date', 'title', 'text', 'link'])\n",
    "for file in [x for x in glob.glob(\"*\") if x.endswith(\".pkl\")]:\n",
    "    with open(file, \"rb\") as f:\n",
    "        articles = pickle.load(f)\n",
    "    name = file.split(\"-\")[0]\n",
    "    df = df.append(pd.DataFrame({'link': articles[0], 'title': articles[1]}))\n",
    "    df.page.fillna(value = name, inplace =True)\n",
    "df.to_csv('data.csv', sep= \";\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unifying individual categories in to single pickle files\n",
    "with open(\"Radio Corporacion_eco-links-titles.pkl\", \"rb\") as f:\n",
    "    articles1 = pickle.load(f)\n",
    "with open(\"Radio Corporacion_nacional-links-titles.pkl\", \"rb\") as f:\n",
    "    articles2 = pickle.load(f)\n",
    "with open(\"Radio Corporacion_politica-links-titles.pkl\", \"rb\") as f:\n",
    "    articles3 = pickle.load(f)\n",
    "    \n",
    "articles[0] = articles1[0] + articles2[0] + articles3[0]\n",
    "articles[1] = articles1[1] + articles2[1] + articles3[1]\n",
    "\n",
    "with open(f'Radio Corporacion.pkl', 'wb') as f:\n",
    "        pickle.dump(articles, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using newsplease to scrape article text and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one goes through articles per outlet. It is designed to be used with multithreading (one thread per outlet)\n",
    "outlets_instructions = [{'name': 'Canal2-links-titles.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': 'Canal6-links-titles.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': 'Canal10-links-titles.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': 'Canal13-links-titles.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': 'Confidencial-links-titles.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': 'Radio Corporacion-links-titles.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': 'Radio la Primerisima-links-titles.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': 'Radio Nicaragua-links-titles.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': '100% Noticias-links-titles.pkl',\n",
    "                         'xpath': \"//div[@class='story-body']\"},\n",
    "                        {'name': 'Radio 800-links-titles.pkl',\n",
    "                         'xpath': \"//div[@class='post-body padd-top']/p\"},\n",
    "                        {'name': 'Canal14-links-titles.pkl',\n",
    "                         'xpath': \"//div[@class='story-body']\"},\n",
    "                        {'name': 'Canal4-links-titles.pkl',\n",
    "                         'xpath': \"//span[@style='color: #000000;']\"}]\n",
    "\n",
    "# made a copy of other instructions, because I need to run scraper again, this time only looking for date\n",
    "outlets_instructions1 = [{'name': 'Canal2-links-titles.pkl_full.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': 'Canal10-links-titles.pkl_full.pkl',\n",
    "                         'xpath': \"//div[@class='date']\"},\n",
    "                        {'name': 'Confidencial-links-titles.pkl_full.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': 'Radio Corporacion-links-titles.pkl_full.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': 'Radio la Primerisima-links-titles.pkl_full.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': 'Radio Nicaragua-links-titles.pkl_full.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': '100% Noticias-links-titles.pkl_full.pkl',\n",
    "                         'xpath': \"//div[@class='story-meta top-meta text-center']/span[2]\"},\n",
    "                        {'name': 'Canal14-links-titles.pkl_full.pkl',\n",
    "                         'xpath': \"//ul[@class='story-meta m-bottom-20']/li[2]\"},\n",
    "                        {'name': 'Canal4-links-titles.pkl_full.pkl',\n",
    "                         'xpath': None}]\n",
    "\n",
    "def scrape_text(outlet):\n",
    "    headers = requests.utils.default_headers()\n",
    "    headers['User-Agent'] = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'\n",
    "    text_list = []\n",
    "    with open(outlet[\"name\"], \"rb\") as f:\n",
    "        articles = pickle.load(f)\n",
    "    logger.info(f'Working on {outlet[\"name\"]}.')\n",
    "    if outlet['xpath'] is None:\n",
    "        for url in articles[0]:\n",
    "            try:\n",
    "                art = NewsPlease.from_url(url)\n",
    "                text = art.maintext\n",
    "                text_list.append(text)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"{outlet['name']}-{url}: {e}\")\n",
    "                text_list.append(None)\n",
    "            sleep(randint(3, 6))\n",
    "    else:\n",
    "        for url in articles[0]:\n",
    "            try:\n",
    "                source = requests.get(url, headers=headers).text\n",
    "                tree = html.fromstring(source)\n",
    "                text = \" \".join([l.text_content() for l in tree.xpath(outlet['xpath'])])\n",
    "                text_list.append(text)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"{outlet['name']}-{url}: {e}\")\n",
    "                text_list.append(None)\n",
    "            sleep(randint(3, 6))\n",
    "    combined = [articles[0], articles[1], text_list]\n",
    "    with open(f'{outlet[\"name\"]}_full.pkl', 'wb') as f:\n",
    "        pickle.dump(combined, f)\n",
    "\n",
    "def scrape_date(outlet):\n",
    "    headers = requests.utils.default_headers()\n",
    "    headers['User-Agent'] = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'\n",
    "    date_list = []\n",
    "    with open(outlet[\"name\"], \"rb\") as f:\n",
    "        articles = pickle.load(f)\n",
    "    logger.warning(f'Working on {outlet[\"name\"]}.')\n",
    "    if outlet['xpath'] is None:\n",
    "        for url in articles[0]:\n",
    "            logger.warning(f'{outlet[\"name\"]}: {url}.')\n",
    "            try:\n",
    "                art = NewsPlease.from_url(url)\n",
    "                date = art.date_publish\n",
    "                date_list.append(date)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"{outlet['name']}-{url}: {e}\")\n",
    "                date_list.append(None)\n",
    "            sleep(randint(3, 6))\n",
    "    else:\n",
    "        for url in articles[0]:\n",
    "            logger.warning(f'{outlet[\"name\"]}: {url}.')\n",
    "            try:\n",
    "                source = requests.get(url, headers=headers).text\n",
    "                tree = html.fromstring(source)\n",
    "                date = \" \".join([l.text_content() for l in tree.xpath(outlet['xpath'])])\n",
    "                date_list.append(date)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"{outlet['name']}-{url}: {e}\")\n",
    "                date_list.append(None)\n",
    "            sleep(randint(3, 6))\n",
    "    combined = [articles[0], articles[1], articles[2], date_list]\n",
    "    with open(f'{outlet[\"name\"]}_full.pkl', 'wb') as f:\n",
    "        pickle.dump(combined, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Scraping only for Canal13, which has so many articles I need to multithread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one was made for a single outlet.\n",
    "# Multithreading should be used here to go through a couple of urls at the same time.\n",
    "def scrape_13(page):\n",
    "    if 'text' not in canal13_list[page]:\n",
    "        if page % 50 == 0: logger.warning(f\"Status: url {page}\")\n",
    "        url = canal13_list[page]['url']\n",
    "        try:\n",
    "            art = NewsPlease.from_url(url)\n",
    "            text = art.maintext\n",
    "            canal13_list[page].update({\"text\": text})\n",
    "        except Exception as e:\n",
    "            text = None\n",
    "            logger.error(f\"{url}: {e}\")\n",
    "        sleep(randint(3, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming to list of dicts\n",
    "with open('Canal10_Accion_10-links-titles.pkl', \"rb\") as f:\n",
    "    articles = pickle.load(f)\n",
    "\n",
    "canal10_list = []\n",
    "for i in range(len(articles[0])):\n",
    "    canal10_list.append({\"url\":articles[0][i], \"title\": articles[1][i]})\n",
    "    \n",
    "with open(\"Canal10-Accion-10-dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(canal10_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will load the canal13 file into memory, work on urls without text and save once interrupted\n",
    "with open(\"Canal13-dict.pkl\", \"rb\") as f:\n",
    "    canal13_list = pickle.load(f)\n",
    "\n",
    "Log_Format = \"%(levelname)s %(asctime)s - %(message)s\"\n",
    "\n",
    "logging.basicConfig(filename = f\"log_canal13.log\",\n",
    "                    format = Log_Format, \n",
    "                    level = logging.WARNING,\n",
    "                    force = True)\n",
    "logger = logging.getLogger()\n",
    "try:\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        executor.map(scrape_13, range(len(canal13_list)))\n",
    "except KeyboardInterrupt:\n",
    "    for i in range(len(canal13_list)):\n",
    "        if 'text' in canal13_list[i]:\n",
    "            pass\n",
    "        else:\n",
    "            logger.warning(f\":Interrupted at {i}\")\n",
    "            break\n",
    "    with open(\"Canal13-dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(canal13_list, f)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking which articles are still missing the text\n",
    "with open(\"Canal13-dict.pkl\", \"rb\") as f:\n",
    "    canal13_list = pickle.load(f)\n",
    "for i in reversed(range(len(canal13_list))):\n",
    "    if 'text' in canal13_list[i]:\n",
    "        pass\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running scraper with logging file without multithreading\n",
    "headers = requests.utils.default_headers()\n",
    "headers['User-Agent'] = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'\n",
    "\n",
    "Log_Format = \"%(levelname)s %(asctime)s - %(message)s\"\n",
    "\n",
    "logging.basicConfig(filename = f\"logfile.log\",\n",
    "                    filemode = \"w\",\n",
    "                    format = Log_Format, \n",
    "                    level = logging.WARNING,\n",
    "                    force = True)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "for x in outlets_instructions1:\n",
    "    scrape_date(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running scraper with logging file with multithreading\n",
    "headers = requests.utils.default_headers()\n",
    "headers['User-Agent'] = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'\n",
    "\n",
    "Log_Format = \"%(levelname)s %(asctime)s - %(message)s\"\n",
    "\n",
    "logging.basicConfig(filename = f\"logfile.log\",\n",
    "                    filemode = \"w\",\n",
    "                    format = Log_Format, \n",
    "                    level = logging.WARNING,\n",
    "                    force = True)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=9) as executor:\n",
    "    executor.map(scrape_100, df100list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"100% Noticias.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df100, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-2b12fd39732b>:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.DataFrame({\"page\":name, \"title\":articles[1],\n",
      "<ipython-input-13-2b12fd39732b>:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.DataFrame({\"page\":name, \"title\":articles[1],\n",
      "<ipython-input-13-2b12fd39732b>:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.DataFrame({\"page\":name, \"title\":articles[1],\n",
      "<ipython-input-13-2b12fd39732b>:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.DataFrame({\"page\":name, \"title\":articles[1],\n",
      "<ipython-input-13-2b12fd39732b>:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.DataFrame({\"page\":name, \"title\":articles[1],\n",
      "<ipython-input-13-2b12fd39732b>:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.DataFrame({\"page\":name, \"title\":articles[1],\n",
      "<ipython-input-13-2b12fd39732b>:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.DataFrame({\"page\":name, \"title\":articles[1],\n",
      "<ipython-input-13-2b12fd39732b>:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.DataFrame({\"page\":name, \"title\":articles[1],\n",
      "<ipython-input-13-2b12fd39732b>:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.DataFrame({\"page\":name, \"title\":articles[1],\n",
      "<ipython-input-13-2b12fd39732b>:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.DataFrame({\"page\":name, \"title\":articles[1],\n",
      "<ipython-input-13-2b12fd39732b>:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.DataFrame({\"page\":name, \"title\":articles[1],\n",
      "<ipython-input-13-2b12fd39732b>:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.DataFrame({\"page\":name, \"title\":articles[1],\n",
      "<ipython-input-13-2b12fd39732b>:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.DataFrame({\"page\":name, \"title\":articles[1],\n",
      "<ipython-input-13-2b12fd39732b>:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.DataFrame({\"page\":name, \"title\":articles[1],\n"
     ]
    }
   ],
   "source": [
    "# putting individual lists of lists into single dataframe\n",
    "df = pd.DataFrame(columns = ['page', 'date', 'title', 'text', 'url'])\n",
    "for file in [x for x in glob.glob(\"*\") if x.endswith(\".pkl\")]:\n",
    "    with open(file, \"rb\") as f:\n",
    "        articles = pickle.load(f)\n",
    "    name = file.split(\".\")[0]\n",
    "    df = df.append(pd.DataFrame({\"page\":name, \"title\":articles[1],\n",
    "                                 \"text\":articles[2], \"date\":articles[3],\n",
    "                                 \"url\":articles[0]}))\n",
    "    \n",
    "df.reset_index(drop = True, inplace = True)\n",
    "df.to_csv(\"dataset.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out xpaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              Thursday 05 de May 2022            \n"
     ]
    }
   ],
   "source": [
    "#for trying stuff out\n",
    "url = \"https://www.canal10.com.ni/noticia/miles-de-excontras-afectados-con-cancelacion-de-organismo-asegura-luis-fley/\"\n",
    "source = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}).text\n",
    "tree = html.fromstring(source)\n",
    "text = \"\\n\".join([l.text_content() for l in tree.xpath(\"//div[@class='date']\")])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Diputados cancelaron la personería jurídica del organismo Fuerza Democrática Nicaragüense, mientras el opositor critica la pasividad con la que ha actuado la comunidad internacional contra el Gobierno de Nicaragua.\\nCon la cancelación de la personería jurídica a la organización no gubernamental Fuerza Democrática Nicaragüense (FDN) por parte de los diputados de la Asamblea Nacional, resultaron afectados más de 25 mil excontras, aseguró el representante del ahora extinto organismo, Luis Fley.\\nFley cuestionó lo que considera una débil presión hacia el Gobierno de Daniel Ortega y Rosario Murillo por parte de la comunidad internacional, lo que ha facilitado que la pareja presidencial continúe en el poder y siga violando los derechos de los nicaragüenses que critican su administración.\\nLea también: Derechos de periodistas en Nicaragua siguen siendo violados, según informe de PCIN\\nEl también exprecandidato presidencial y opositor criticó a los países que conforman la Organización de Estados Americanos y sus directivos por no ejecutar medidas significativas en contra del Gobierno de Nicaragua.\\nLa Contra fue un movimiento armado que surgió para combatir al Gobierno sandinista en la década de los 80, pero decidieron desarmarse cuando triunfó doña Violeta Barrios de Chamorro en las elecciones de 1990. Este miércoles los diputados de la Asamblea Nacional cancelaron la personería jurídica a 50 organizaciones de la sociedad civil, afectando a sectores vulnerables de la sociedad nicaragüense.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.canal10.com.ni/noticia/miles-de-excontras-afectados-con-cancelacion-de-organismo-asegura-luis-fley/\"\n",
    "art = NewsPlease.from_url(url)\n",
    "text = art.maintext\n",
    "text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
