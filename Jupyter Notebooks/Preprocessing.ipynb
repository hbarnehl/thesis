{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import dateparser\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-46384b245f3e>:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df10.loc[:,'date'] = df10['date'].str.replace('de ', '', regex=True)\n",
      "<ipython-input-7-46384b245f3e>:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df10.loc[:,'date'] = df10['date'].str.replace(r'Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday', '', regex=True)\n",
      "<ipython-input-7-46384b245f3e>:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df10.loc[:,'date'] = pd.to_datetime(df10['date'])\n",
      "/home/hennes/.local/lib/python3.8/site-packages/dateparser/date_parser.py:35: PytzUsageWarning: The localize method is no longer necessary, as this time zone supports the fold attribute (PEP 495). For more details on migrating to a PEP 495-compliant implementation, see https://pytz-deprecation-shim.readthedocs.io/en/latest/migration.html\n",
      "  date_obj = stz.localize(date_obj)\n"
     ]
    }
   ],
   "source": [
    "dataloc = \"/home/hennes/thesis/Data/\"\n",
    "df = pd.read_csv(dataloc+\"dataset.csv\")\n",
    "\n",
    "### Some preliminary stuff\n",
    "\n",
    "# unify outlet names\n",
    "df.loc[df[\"page\"].str.startswith('Canal13'), \"page\"] = \"Canal13\"\n",
    "df.loc[df[\"page\"].str.startswith('100% Noticias'), \"page\"] = \"100% Noticias\"\n",
    "df.loc[df[\"page\"].str.startswith('Confidencial'), \"page\"] = \"Confidencial\"\n",
    "df.loc[df[\"page\"].str.startswith('Radio Corporacion'), \"page\"] = \"Radio Corporacion\"\n",
    "df.loc[df[\"page\"].str.startswith('Canal10'), \"page\"] = \"Canal10\"\n",
    "df.loc[df[\"page\"].str.startswith('Canal14'), \"page\"] = \"Canal14\"\n",
    "\n",
    "\n",
    "\n",
    "# extract canal13 dates \n",
    "df.loc[df[\"page\"] == 'Canal13', [\"date\"]] = df.loc[df[\"page\"] == 'Canal13',\n",
    "                                                 \"url\"].str.extract(r'(\\d\\d\\d\\d/\\d\\d/\\d\\d)').values\n",
    "# extract canal 6 dates\n",
    "df.loc[df[\"page\"] == 'Canal6', [\"date\"]] = df.loc[df[\"page\"] == 'Canal6',\n",
    "\n",
    "                                                \"url\"].str.extract(r'(\\d\\d\\d\\d/\\d\\d/\\d\\d)').values\n",
    "# extract radio 800 dates\n",
    "df.loc[df[\"page\"] == 'Radio 800', [\"date\"]] = df.loc[df[\"page\"] == 'Radio 800',\n",
    "                                                   \"url\"].str.extract(r'(\\d\\d\\d\\d/\\d\\d/\\d\\d)').values\n",
    "\n",
    "# convert canal10 date to datetime\n",
    "df10 = df.loc[df[\"page\"] == \"Canal10\"]\n",
    "df10.loc[:,'date'] = df10['date'].str.replace('de ', '', regex=True)\n",
    "df10.loc[:,'date'] = df10['date'].str.replace(r'Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday', '', regex=True)\n",
    "df10.loc[:,'date'] = pd.to_datetime(df10['date'])\n",
    "df.loc[df[\"page\"] == \"Canal10\", 'date'] = df10[\"date\"].to_list()\n",
    "\n",
    "# convert canal14 date to datetime\n",
    "# first delete entries where only time of day was extracted\n",
    "df.loc[df[\"page\"] == \"Canal14\"] = df.loc[(df[\"page\"] == \"Canal14\") & (df[\"date\"].str.contains(r\"\\d{4}\", na=False))]\n",
    "# I am doing that with dateparser, because pandas cannot handle the spanish dates\n",
    "df14 = df.loc[df[\"page\"] == \"Canal14\"]\n",
    "df14.loc[:,'date'] = df14.loc[:,'date'].apply(lambda x: dateparser.parse(x, settings={'STRICT_PARSING': True}))\n",
    "df.loc[df[\"page\"] == \"Canal14\", 'date'] = df14[\"date\"].to_list()\n",
    "\n",
    "# 100 % noticias\n",
    "df100 = df.loc[df[\"page\"] == \"100% Noticias\"]\n",
    "# convert nan in date column to empty string so that dateparser works\n",
    "df100.fillna({\"date\":\" \"}, axis=0, inplace=True)\n",
    "df100.loc[:,'date'] = df100.loc[:,'date'].apply(lambda x: dateparser.parse(x, settings={'STRICT_PARSING': True}))\n",
    "df.loc[df[\"page\"] == \"100% Noticias\", 'date'] = df100[\"date\"].to_list()\n",
    "\n",
    "# canal2, canal4, confidencial, radio corporacion, radio nicaragua,\n",
    "# radio primerissima are already in datetime format\n",
    "\n",
    "# convert the rest to datetime\n",
    "df.loc[:,\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "# create year variable\n",
    "df[\"year\"] = pd.DatetimeIndex(df.date).year\n",
    "\n",
    "df.to_csv(\"dataset1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Tokenization Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles without text per outlet\n",
      "page\n",
      "100% Noticias                 29\n",
      "Canal10                     1070\n",
      "Canal13                     9935\n",
      "Canal2.pkl                    31\n",
      "Canal4.pkl                     2\n",
      "Canal6.pkl                    10\n",
      "Radio Corporacion            119\n",
      "Radio Nicaragua.pkl          111\n",
      "Radio la Primerisima.pkl       7\n",
      "dtype: int64\n",
      "\n",
      "Number of articles without text per year\n",
      "year\n",
      "2011.0      72\n",
      "2012.0    1611\n",
      "2013.0    3408\n",
      "2014.0    1874\n",
      "2015.0    2068\n",
      "2016.0    1152\n",
      "2017.0     270\n",
      "2018.0     292\n",
      "2019.0     108\n",
      "2020.0     389\n",
      "2021.0      51\n",
      "2022.0       4\n",
      "dtype: int64\n",
      "year                      2011.0  2012.0  2013.0  2014.0  2015.0  2016.0  \\\n",
      "page                                                                       \n",
      "100% Noticias                NaN     NaN     NaN     NaN  1742.0  3236.0   \n",
      "Canal10                      NaN     NaN     NaN     NaN     NaN  3517.0   \n",
      "Canal13                    167.0   550.0     6.0  1820.0  2162.0  4963.0   \n",
      "Canal14                      NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "Canal2.pkl                   NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "Canal4.pkl                   NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "Confidencial                 NaN     NaN     NaN     NaN   384.0   783.0   \n",
      "Radio Corporacion            NaN     NaN     NaN     NaN    62.0   490.0   \n",
      "Radio Nicaragua.pkl          NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "Radio la Primerisima.pkl     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "\n",
      "year                      2017.0  2018.0  2019.0  2020.0   2021.0  2022.0  \n",
      "page                                                                       \n",
      "100% Noticias              903.0  3065.0  1265.0  3813.0   3882.0   491.0  \n",
      "Canal10                   3104.0  1710.0  1556.0  6465.0   2867.0   750.0  \n",
      "Canal13                   5820.0  6678.0  5195.0  3917.0   3478.0   453.0  \n",
      "Canal14                     59.0  1416.0  1263.0  1508.0   1376.0   581.0  \n",
      "Canal2.pkl                   NaN     NaN     NaN   672.0   1722.0   198.0  \n",
      "Canal4.pkl                 656.0  6811.0  3219.0  1150.0   3136.0   413.0  \n",
      "Confidencial               631.0  1190.0  1175.0  1724.0   1972.0   237.0  \n",
      "Radio Corporacion          499.0  1592.0  1720.0  1767.0   2607.0   319.0  \n",
      "Radio Nicaragua.pkl          NaN  1261.0  2032.0  9004.0   7132.0   999.0  \n",
      "Radio la Primerisima.pkl     NaN     NaN     NaN  2217.0  10670.0  1866.0  \n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv(\"dataset1.csv\")\n",
    "\n",
    "# delete duplicate articles\n",
    "df.drop_duplicates(subset=\"title\", inplace=True, ignore_index=True)\n",
    "\n",
    "# check for articles that do not have any text in them\n",
    "print(\"Number of articles without text per outlet\")\n",
    "print(df[df.text.isna()].groupby('page').size())\n",
    "\n",
    "# there are quite some articles without text. I randomly selected some of these to verify that\n",
    "# this is not a mistake in my crawler, but that they just don't have text. Indeed, they are all\n",
    "# articles that only contain videos. This is reflected by the fact that the outlets in thise list\n",
    "# are mostly TV and radio stations.\n",
    "\n",
    "print(\"\\nNumber of articles without text per year\")\n",
    "print(df[df.text.isna()].groupby('year').size())\n",
    "\n",
    "# Because the models need text to work, I will discard rows without text\n",
    "\n",
    "df = df.loc[~df.text.isna()].reset_index(drop = True)\n",
    "\n",
    "# there is also a discrepancy in when the outlets started their work, as can be seen from the following table\n",
    "print(df.groupby(['year', \"page\"]).agg({\"text\":np.size}).reset_index().pivot(\"page\", \"year\", \"text\"))\n",
    "\n",
    "# Because before 2015, only a single government outlet uploaded articles, I will keep only articles published\n",
    "# from 2015 on\n",
    "\n",
    "df = df.loc[df[\"year\"]>2014]\n",
    "\n",
    "# removing boilerplate, news agency sources, hyperlinks etc.\n",
    "\n",
    "             # newlines, tabs etc.\n",
    "repl_dict = {r\"\\t|\\n|\\r|\\xa0\":\" \",\n",
    "             # whitespace\n",
    "             r\"\\s{2,}\":\" \"}\n",
    "df.replace({\"text\":repl_dict, \"title\":repl_dict}, regex=True, inplace = True)\n",
    "\n",
    "            # hashtags\n",
    "repl_dict = {\"#\": \"\",\n",
    "             # source in some articles\n",
    "            r'Fuente: El 19 Digital|Fuente: TN8':\"\",\n",
    "             # all hyperlinks\n",
    "            r\"http\\S+\":\"\",\n",
    "             # canal10 boilerplate\n",
    "            r\"Foto: Shutterstock.*\": \"\",\n",
    "            r\"Foto:.*{.*\": \"\",\n",
    "            \"Noticias de Nicaragua y el Mundo\": \"\",\n",
    "            r\"p(\\.\\w+\\s|\\s)\\{.*?\\}\": \"\",\n",
    "            r\"\\{.*?\\}\": \"\",\n",
    "            r\"Normal.*X-NONE( /\\* Style Definitions \\*/ table\\.MsoNormalTable)?\": \"\",\n",
    "             # canal4 boilerplate\n",
    "            r\"Canal 4 Noticias[\\s\\S]+Canal 4 Nicaragua. Todos los derechos reservados\": \"\",\n",
    "            r\"Comparte[.\\s]*?esto:[.\\s]*?Tweet[.\\s]*?WhatsApp[.\\s]*?Telegram\": \"\",\n",
    "            \"LEER TAMBIÉN\": \"\",\n",
    "            \"Leer más:\": \"\",\n",
    "            \"AMPLIACIÓN EN BREVE…\": \"\",\n",
    "            r\"Canal 4 Noticias de Nicaragua.*\": \"\",\n",
    "             \"Periodista en Multinoticias, Canal 4\": \"\",\n",
    "             # news agency\n",
    "            \"(EFE)\":\"\",\n",
    "             # source mentioned at end of article\n",
    "            r\"Con información de\\:+$\": \"\",\n",
    "             # copyright stuff\n",
    "            \"© 100% Noticias ¡Con primicias a toda hora!\":\"\",\n",
    "            \"© Getty Images\":\"\",\n",
    "            r\"©\\s?[Vv]iva [Nn]icaragua,? (Canal 13 )?(Previous Next)?\": \"\",\n",
    "            \"© AFP\":\"\",\n",
    "            \"© AP\":\"\",\n",
    "            \"© creative commons\": \"\",\n",
    "            \"© El 19 Digital\": \"\",\n",
    "            \"© Consejo de Comunicación y Ciudadanía\": \"\",\n",
    "            \"© Juventud Presidente\": \"\",\n",
    "            \"© Ministerio de Gobernación\": \"\",\n",
    "             # copyrights for photographers\n",
    "            r\"\\w+?\\s+?\\w+?\\s+?©\":\"\",\n",
    "             # article suggestions\n",
    "            \"Te recomendamos:\": \"\",\n",
    "            \"Quizás te interesa:\":\"\",\n",
    "            \"Lee Aquí:\": \"\",\n",
    "             # twitter links\n",
    "            r\"pic\\.twitter\\.com.+?\\d{4}\": \"\",\n",
    "            r\"—.+?\\(@.+?\\).+?\\d{4}\":\"\",\n",
    "             # source information\n",
    "            r\"Con información de:.+$\": \"\",\n",
    "            # emails\n",
    "            r\"\\[email protected\\]\":\"\"}\n",
    "\n",
    "df.replace({\"text\":repl_dict, \"title\":repl_dict}, regex=True, inplace = True)\n",
    "\n",
    "# the same goes for articles with very short texts.\n",
    "# again, due to the focus on videos, some articles in the dataset consist only\n",
    "# of a single, short sentence. Articles with less than 100-character long texts will\n",
    "# therefore likewise be deleted.\n",
    "\n",
    "df = df.loc[df[\"text\"].apply(lambda x: len(str(x)) > 200)].reset_index(drop = True)\n",
    "\n",
    "df.to_csv(\"dataset_token_ready.csv\", index = False)\n",
    "\n",
    "# save text files separately as csv file\n",
    "df[\"text\"].to_csv(\"texts.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import gensim\n",
    "import csv\n",
    "\n",
    "# save only text as csv file\n",
    "df = pd.read_csv(\"dataset_token_ready.csv\")[\"text\"]\n",
    "\n",
    "df.to_csv(\"texts.csv\", index = False)\n",
    "\n",
    "# load text as generator\n",
    "csv_gen = (row for row in open(\"texts.csv\"))\n",
    "\n",
    "# load text as list\n",
    "textlist = pd.read_csv(\"dataset_token_ready.csv\")[\"text\"].tolist()\n",
    "\n",
    "# load spanish language model\n",
    "nlp = spacy.load('es_core_news_md', disable=[\"parser\", \"attribute_ruler\", \"ner\"])\n",
    "# add stopwords\n",
    "nlp.Defaults.stop_words |= {\"a\",\"y\", \"o\"}\n",
    "# reload language model to incorporate new stopwords \n",
    "nlp = spacy.load('es_core_news_md', disable=[\"parser\", \"attribute_ruler\", \"ner\"])\n",
    "\n",
    "tokenlist = []\n",
    "# go through rows of article texts\n",
    "for i, doc in enumerate(nlp.pipe(csv_gen, disable=[\"parser\", \"attribute_ruler\", \"ner\"], n_process=4)):\n",
    "    # append lists of lemmatised tokens to tokenlist\n",
    "    tokenlist.append([token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha])\n",
    "    if i % 10000 == 0 and i != 0: print(i)\n",
    "# remove first entry, which is the column title\n",
    "tokenlist = tokenlist[1:]\n",
    "# save to pickle\n",
    "with open(f\"tokens/tokens.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenlist, f)\n",
    "    \n",
    "# integrate tokenlist into df, useful for later structural topic model and all subsampling\n",
    "# load tokenlist\n",
    "with open(\"tokens/tokens.pkl\", \"rb\") as f:\n",
    "    tokenlist = pickle.load(f)\n",
    "\n",
    "# integrate tokenlist back into dataframe\n",
    "df = pd.read_csv(\"dataset_token_ready.csv\")\n",
    "df = df.assign(tokens=tokenlist)\n",
    "\n",
    "# save dataset as pickle (csv does not work, because it cannot save the lists of tokens as cell entries)\n",
    "df.to_pickle(\"dataset_with_tokens.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select tokens and subsample\n",
    "tokenlist = pd.read_pickle(\"dataset_with_tokens.pkl\")[[\"page\", \"tokens\"]]\n",
    "tokenlist = tokenlist[\"tokens\"].tolist()\n",
    "\n",
    "# make bigram and trigram mod\n",
    "bigram = gensim.models.Phrases(tokenlist, min_count = 100, threshold = 100)\n",
    "trigram = gensim.models.Phrases(bigram[tokenlist], threshold = 130)\n",
    "\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "tokenlist = [trigram_mod[bigram_mod[text]] for text in tokenlist]\n",
    "\n",
    "tokens_trigrams = []\n",
    "for doc in tokenlist:\n",
    "    tokenstring = [\" \".join([token for token in doc])]\n",
    "    tokens_trigrams.append(tokenstring)\n",
    "\n",
    "with open(\"tokens/tokens_trigrams.csv\", \"w\") as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerows(tokens_trigrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
