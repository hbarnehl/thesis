{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping Thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from lxml import html\n",
    "from random import randint\n",
    "from time import sleep\n",
    "import pickle\n",
    "import concurrent\n",
    "import logging\n",
    "import glob\n",
    "from newsplease import NewsPlease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following dictionaries, I collect the information necessary to loop over the news pages. In the first dictionary, each entry specifies the name of the news outlet, the url, the number of pages, the xpath to access article titles and the xpath to access the article links. In some cases, the links only contain a part of the link. In those cases, the necessary prefix is included as a key too. In some cases, there are two xpaths which need to be accessed to retrieve all relevant titles and links. In those cases, title and link paths are recorded in a list.\n",
    "\n",
    "The second dictionary contains information on whether the text can be downloaded with newsplease or not. If it cannot, the dictionary provides the xpath of the text and date location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [{'name':'Canal2',\n",
    "        'url':'https://canal2tv.com/category/nacionales/page/',\n",
    "        'pages':132,\n",
    "        'titlepath': \"//div[@class='post-container']//a[@class='post-title']/h2\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//div[@class='post-container']//a[@class='post-title']\"},\n",
    "        {'name':'Canal4',\n",
    "        'url':'https://www.canal4.com.ni/nicaragua/page/',\n",
    "        'pages':1565,\n",
    "        'titlepath': \"//div[@class='tg-col-control']//h3/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//div[@class='tg-col-control']//h3/a\"},\n",
    "        {'name':'Canal6',\n",
    "        'url':'https://canal6.com.ni/category/nacionales/page/',\n",
    "        'pages':307,\n",
    "        'titlepath': \"//figure[@class='figure']//a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//figure[@class='figure']//a\"},\n",
    "        {'name':'Canal10',\n",
    "        'url':'https://www.canal10.com.ni/category/nacionales/page/',\n",
    "        'pages':1234,\n",
    "        'titlepath': \"//div[@class='item card-type-a child']//h2/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//div[@class='item card-type-a child']//h2/a\"},\n",
    "         {'name':'Canal10_Accion_10',\n",
    "        'url':'https://www.canal10.com.ni/category/accion-10/page/',\n",
    "        'pages':4014,\n",
    "        'titlepath': \"//div[@class='item card-type-a child']//h2/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//div[@class='item card-type-a child']//h2/a\"},\n",
    "        {'name':'Canal13_politica',\n",
    "        'url':'https://www.vivanicaragua.com.ni/category/politica/page/',\n",
    "        'pages':445,\n",
    "        'titlepath': \"//a[@class='card-title']//h3\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//a[@class='card-title']\"},\n",
    "        {'name':'Canal13_economia',\n",
    "        'url':'https://www.vivanicaragua.com.ni/category/economia/page/',\n",
    "        'pages':363,\n",
    "        'titlepath': \"//a[@class='card-title']//h3\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//a[@class='card-title']\"},\n",
    "        {'name':'Canal13_sociales',\n",
    "        'url':'https://www.vivanicaragua.com.ni/category/sociales/page/',\n",
    "        'pages':2997,\n",
    "        'titlepath': \"//a[@class='card-title']//h3\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//a[@class='card-title']\"},\n",
    "        {'name':'Canal14',\n",
    "        'url':'https://www.vostv.com.ni/nacionales/?page=',\n",
    "        'pages':669,\n",
    "        'titlepath': \"//section[@class='secondary-news']//h3\",\n",
    "        'prefix': 'https://www.vostv.com.ni',\n",
    "        'linkpath': \"//section[@class='secondary-news']//div[@class='figure-cap']/a[1]\"},\n",
    "        {'name':'Radio la Primerisima',\n",
    "        'url':'https://radiolaprimerisima.com/noticias-generales/page/',\n",
    "        'pages':797,\n",
    "        'titlepath': \"//div[@class='post_title']//a/span[1]\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//div[@class='post_title']//a\"},\n",
    "        {'name':'La Nueva Radio Ya',\n",
    "        'url':'https://nuevaya.com.ni/nacionales/page/',\n",
    "        'pages':1430,\n",
    "        'titlepath': \"//div[@class='vc_column tdi_52 wpb_column vc_column_container tdc-column td-pb-span9']//h3[@class='entry-title td-module-title']//a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//div[@class='vc_column tdi_52 wpb_column vc_column_container tdc-column td-pb-span9']//h3[@class='entry-title td-module-title']//a\"},\n",
    "        {'name':'Radio 800',\n",
    "        'url':'https://radio800ni.com/category/nacionales/page/',\n",
    "        'pages':81,\n",
    "        'titlepath': \"//h2[@class='post-title']/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//h2[@class='post-title']/a\"},\n",
    "        {'name':'Radio Nicaragua',\n",
    "        'url':'https://radionicaragua.com.ni/category/nacionales/page/',\n",
    "        'pages':2161,\n",
    "        'titlepath': \"//figcaption/a/h2\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//figcaption/a\"},\n",
    "        {'name':'Radio Corporacion_nacional',\n",
    "        'url':'https://radio-corporacion.com/blog/archivos/category/nacional/page/',\n",
    "        'pages':584,\n",
    "        'titlepath': \"//h3[@class='mh-loop-title']/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//h3[@class='mh-loop-title']/a\"},\n",
    "        {'name':'Radio Corporacion_politica',\n",
    "        'url':'https://radio-corporacion.com/blog/archivos/category/politica/page/',\n",
    "        'pages':264,\n",
    "        'titlepath': \"//h3[@class='mh-loop-title']/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//h3[@class='mh-loop-title']/a\"},\n",
    "        {'name':'Radio Corporacion_eco',\n",
    "        'url':'https://radio-corporacion.com/blog/archivos/category/eco/page/',\n",
    "        'pages':116,\n",
    "        'titlepath': \"//h3[@class='mh-loop-title']/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//h3[@class='mh-loop-title']/a\"},\n",
    "        {'name':'Confidencial_politica',\n",
    "        'url':'https://www.confidencial.com.ni/politica/page/',\n",
    "        'pages':355,\n",
    "        'titlepath': \"//h2[@class='archive-titles']/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//h2[@class='archive-titles']/a\"},\n",
    "        {'name':'Confidencial_economia',\n",
    "        'url':'https://www.confidencial.com.ni/economia/page/',\n",
    "        'pages':168,\n",
    "        'titlepath': \"//h2[@class='archive-titles']/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//h2[@class='archive-titles']/a\"},\n",
    "        {'name':'Confidencial_nacion',\n",
    "        'url':'https://www.confidencial.com.ni/nacion/page/',\n",
    "        'pages':637,\n",
    "        'titlepath': \"//h2[@class='archive-titles']/a\",\n",
    "        'prefix': \"\",\n",
    "        'linkpath': \"//h2[@class='archive-titles']/a\"},\n",
    "        {'name':'100% Noticias_nacionales',\n",
    "        'url':'https://100noticias.com.ni/nacionales/?page=',\n",
    "        'pages':747,\n",
    "        'titlepath': [\"//div[@class='col-md-6 m-bottom-10']//a//h5\", \"//div[@class='col-6 col-md-4']/a//h5\"],\n",
    "        'prefix' : \"https://100noticias.com.ni\",\n",
    "        'linkpath': [\"//div[@class='col-md-6 m-bottom-10']//a\", \"//div[@class='col-6 col-md-4']/a\"]},\n",
    "        {'name':'100% Noticias_economia',\n",
    "        'url':'https://100noticias.com.ni/economia/?page=',\n",
    "        'pages':73,\n",
    "        'titlepath': [\"//div[@class='col-md-6 m-bottom-10']//a//h5\", \"//div[@class='col-6 col-md-4']/a//h5\"],\n",
    "        'prefix' : \"https://100noticias.com.ni\",\n",
    "        'linkpath': [\"//div[@class='col-md-6 m-bottom-10']//a\", \"//div[@class='col-6 col-md-4']/a\"]},\n",
    "        {'name':'100% Noticias_politica',\n",
    "        'url':'https://100noticias.com.ni/politica/?page=',\n",
    "        'pages':114,\n",
    "        'titlepath': [\"//div[@class='col-md-6 m-bottom-10']//a//h5\", \"//div[@class='col-6 col-md-4']/a//h5\"],\n",
    "        'prefix': \"https://100noticias.com.ni\",\n",
    "        'linkpath': [\"//div[@class='col-md-6 m-bottom-10']//a\", \"//div[@class='col-6 col-md-4']/a\"]}]\n",
    "\n",
    "outlets_instructions = [{'name': 'Canal2-links-titles.pkl',\n",
    "                         'xpath': None,\n",
    "                        'date': None},\n",
    "                        {'name': 'Canal6-links-titles.pkl',\n",
    "                         'xpath': None,\n",
    "                        'date':None},\n",
    "                        {'name': 'Canal10-links-titles.pkl',\n",
    "                         'xpath': None,\n",
    "                        'date': \"//div[@class='date']\"},\n",
    "                        {'name': 'Canal13-links-titles.pkl',\n",
    "                         'xpath': None},\n",
    "                        {'name': 'Confidencial-links-titles.pkl',\n",
    "                         'xpath': None,\n",
    "                        'date': None},\n",
    "                        {'name': 'Radio Corporacion-links-titles.pkl',\n",
    "                         'xpath': None,\n",
    "                        'date': None},\n",
    "                        {'name': 'Radio la Primerisima-links-titles.pkl',\n",
    "                         'xpath': None,\n",
    "                        'date': None},\n",
    "                        {'name': 'Radio Nicaragua-links-titles.pkl',\n",
    "                         'xpath': None,\n",
    "                        'date': None},\n",
    "                        {'name': '100% Noticias-links-titles.pkl',\n",
    "                         'xpath': \"//div[@class='story-body']\",\n",
    "                        'date': \"//div[@class='story-meta top-meta text-center']/span[2]\"},\n",
    "                        {'name': 'Canal14-links-titles.pkl',\n",
    "                         'xpath': \"//div[@class='story-body']\",\n",
    "                        'date': \"//ul[@class='story-meta m-bottom-20']/li[2]\"},\n",
    "                        {'name': 'Canal4-links-titles.pkl',\n",
    "                         'xpath': \"//span[@style='color: #000000;']\",\n",
    "                        'date': None}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_main_pages(outlet):\n",
    "    '''This function loops through the pages of the main page of the provided outlet.\n",
    "        It downloads the html of the sites, appends them to a list and saves that\n",
    "        list once it is done.'''\n",
    "    htmllist = []\n",
    "    headers = requests.utils.default_headers()\n",
    "    headers['User-Agent'] = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'\n",
    "    baseurl = outlet['url']\n",
    "    name = outlet['name']\n",
    "    \n",
    "    logger.info(f'Working on {name}.')\n",
    "    \n",
    "     # start looping through pages\n",
    "    for i in range(1, outlet['pages']+1):\n",
    "        # report on status at every ten pages\n",
    "        if i % 10 == 0: logger.info(f\"Status {name}: page {i}\")\n",
    "        try:\n",
    "            url = baseurl+str(i)\n",
    "            source = requests.get(url, headers=headers).text\n",
    "            tree = html.fromstring(source)\n",
    "            htmllist.append(tree)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error with {name} at page {i}:\")\n",
    "            logger.error(e)\n",
    "        sleep(randint(3, 6))\n",
    "    \n",
    "    with open(f'{name}-html.pkl', 'wb') as f:\n",
    "        pickle.dump(htmllist, f)\n",
    "        \n",
    "def scrape_articles(outlet):\n",
    "    '''This funtion loops throught the htmllist of the input outlet and extracts all article\n",
    "    links and titles. These are then saved in two lists, which are then combined in a list of\n",
    "    lists and saved in pickle format.'''\n",
    "    linklist = []\n",
    "    titlelist = []\n",
    "    with open(f'html/{name}-html.pkl', 'rb') as f:\n",
    "        htmllist = pickle.load(f)\n",
    "    \n",
    "    # start looping through pages\n",
    "    for i in htmllist:\n",
    "        try:\n",
    "            tree = i\n",
    "            if isinstance(outlet['titlepath'] , list):\n",
    "                links = [outlet['prefix'] + l.attrib['href'] for l in (tree.xpath(outlet[\"linkpath\"][0]) + tree.xpath(outlet[\"linkpath\"][1]))]\n",
    "                titles = [l.text for l in (tree.xpath(outlet[\"titlepath\"][0]) + tree.xpath(outlet[\"titlepath\"][1]))]\n",
    "            else:\n",
    "                links = [outlet['prefix'] + l.attrib['href']for l in tree.xpath(outlet[\"linkpath\"])]\n",
    "                titles = [l.text for l in tree.xpath(outlet[\"titlepath\"])]\n",
    "            [linklist.append(x) for x in links]\n",
    "            [titlelist.append(x) for x in titles]\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name} at page {i}:\")\n",
    "            print(e)\n",
    "    \n",
    "    combined = [linklist, titlelist]\n",
    "    with open(f'data/{name}-links-titles.pkl', 'wb') as f:\n",
    "        pickle.dump(combined, f)\n",
    "\n",
    "def download_articles(outlet):\n",
    "    '''This function loops through the articles of one outlet and downloads them. It then\n",
    "    appends them to a list and saves that list once it is done.'''\n",
    "    headers = requests.utils.default_headers()\n",
    "    headers['User-Agent'] = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'\n",
    "    htmllist = []\n",
    "    name = outlet[\"name\"].split(\"-\")[0]\n",
    "    \n",
    "    with open(outlet[\"name\"], \"rb\") as f:\n",
    "        articles = pickle.load(f)\n",
    "    logger.info(f'Working on {outlet[\"name\"]}.')\n",
    "    if outlet['xpath'] is None:\n",
    "        for url in articles[0]:\n",
    "            try:\n",
    "                art = NewsPlease.from_url(url)\n",
    "                htmllist.append(tree)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"{outlet['name']}-{url}: {e}\")\n",
    "            sleep(randint(1, 4))\n",
    "    else:\n",
    "        for url in articles[0]:\n",
    "            try:\n",
    "                source = requests.get(url, headers=headers).text\n",
    "                tree = html.fromstring(source)\n",
    "                htmllist.append(tree)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"{outlet['name']}-{url}: {e}\")\n",
    "            sleep(randint(1, 4))\n",
    "            \n",
    "    with open(f'html/{name}-articles-html.pkl', 'wb') as f:\n",
    "        pickle.dump(htmllist, f)\n",
    "\n",
    "def scrape_text_date(outlet):\n",
    "    '''This funtion loops throught the htmllist of the input outlet and extracts all text,\n",
    "    and dates. These are then saved in two lists, which are then combined in a list of\n",
    "    lists and saved in pickle format.'''\n",
    "    text_list = []\n",
    "    date_list = []\n",
    "    name = outlet[\"name\"].split(\"-\")[0]\n",
    "    with open(f\"html/{name}-articles-html.pkl\", \"rb\") as f:\n",
    "        htmllist = pickle.load(f)\n",
    "    \n",
    "    # scrape the text with newsplease if possible\n",
    "    if outlet['xpath'] is None:\n",
    "        for html in htmllist:\n",
    "            try:\n",
    "                art = html\n",
    "                text = art.maintext\n",
    "                text_list.append(text)\n",
    "            except Exception as e:\n",
    "                print(f\"{outlet['name']}-{url}: {e}\")\n",
    "                text_list.append(None)\n",
    "                \n",
    "    # otherwise use the xpath\n",
    "    else:\n",
    "        for html in htmllist:\n",
    "            try:\n",
    "                tree = html\n",
    "                text = \" \".join([l.text_content() for l in tree.xpath(outlet['xpath'])])\n",
    "                text_list.append(text)\n",
    "            except Exception as e:\n",
    "                print(f\"{outlet['name']}-{url}: {e}\")\n",
    "                text_list.append(None)\n",
    "    \n",
    "    # download the date with newsplease if possible\n",
    "    if outlet['date'] is None:\n",
    "        for html in htmllist:\n",
    "            try:\n",
    "                art = html\n",
    "                date = art.date_publish\n",
    "                date_list.append(date)\n",
    "            except Exception as e:\n",
    "                print(f\"{outlet['name']}-{url}: {e}\")\n",
    "                date_list.append(None)\n",
    "            sleep(randint(3, 6))\n",
    "    else:\n",
    "        for html in htmllist:\n",
    "            try:\n",
    "                tree = html\n",
    "                date = \" \".join([l.text_content() for l in tree.xpath(outlet['xpath'])])\n",
    "                date_list.append(date)\n",
    "            except Exception as e:\n",
    "                print(f\"{outlet['name']}-{url}: {e}\")\n",
    "                date_list.append(None)\n",
    "                \n",
    "    with open(outlet[\"name\"], \"rb\") as f:\n",
    "        articles = pickle.load(f)\n",
    "    link_list = articles[0]\n",
    "    title_list = articles[1]\n",
    "    \n",
    "    combined = [link_list, title_list, text_list, date_list]\n",
    "    with open(f'data/{outlet[\"name\"]}_full.pkl', 'wb') as f:\n",
    "        pickle.dump(combined, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the HTML of the main pages and scrape them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Canal10_Accion_10',\n",
       " 'url': 'https://www.canal10.com.ni/category/accion-10/page/',\n",
       " 'pages': 4014,\n",
       " 'titlepath': \"//div[@class='item card-type-a child']//h2/a\",\n",
       " 'prefix': '',\n",
       " 'linkpath': \"//div[@class='item card-type-a child']//h2/a\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating and Configuring Logger\n",
    "Log_Format = \"%(levelname)s %(asctime)s - %(message)s\"\n",
    "\n",
    "logging.basicConfig(filename = f\"logfile.log\",\n",
    "                    filemode = \"w\",\n",
    "                    format = Log_Format, \n",
    "                    level = logging.INFO,\n",
    "                    force = True)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# download htmls of all outlets at the same time\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=len(pages)) as executor:\n",
    "    executor.map(download_main_pages, pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape four outlets at the same time\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:\n",
    "    executor.map(scrape_articles, pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine individual outlet categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get set of unique outlets\n",
    "namelist = [e.split(\"-\")[0] for e glob.glob(\"data/*\")]\n",
    "namelist = set(namelist)\n",
    "\n",
    "# get all files\n",
    "all_files = glob.glob(\"data/*\")\n",
    "\n",
    "# for each unique outlet\n",
    "for name in namelist:\n",
    "    links = []\n",
    "    art_name = []\n",
    "    \n",
    "    # find all files that pertain to outlet\n",
    "    files = [f for f in all_files if f.split(\"/\")[1].startswith(name)]\n",
    "    \n",
    "    # for each of them, combine all links and article names in single file\n",
    "    for file in files:\n",
    "        with open(file, \"rb\") as f:\n",
    "            articles = pickle.load(f)\n",
    "            links.append(articles[0])\n",
    "            art_name.append(articles[1])\n",
    "    combined = [links, art_name]\n",
    "    \n",
    "    # save that file\n",
    "    with open(f'data/{name}-links-titles.pkl', 'wb') as f:\n",
    "        pickle.dump(combined, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Scrape individual Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating and Configuring Logger\n",
    "Log_Format = \"%(levelname)s %(asctime)s - %(message)s\"\n",
    "\n",
    "logging.basicConfig(filename = f\"logfile.log\",\n",
    "                    filemode = \"w\",\n",
    "                    format = Log_Format, \n",
    "                    level = logging.INFO,\n",
    "                    force = True)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# download htmls of all outlets at the same time\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=len(pages)) as executor:\n",
    "    executor.map(download_articles, pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape four outlets at the same time\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:\n",
    "    executor.map(scrape_text_date, pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting individual lists of lists into single dataframe\n",
    "df = pd.DataFrame(columns = ['page', 'date', 'title', 'text', 'url'])\n",
    "\n",
    "# for each of full outlet files\n",
    "for file in [x for x in glob.glob(\"data/*\") if x.endswith(\"._full.pkl\")]:\n",
    "    with open(file, \"rb\") as f:\n",
    "        articles = pickle.load(f)\n",
    "    name = file.split(\".\")[0]\n",
    "    # append to dataframe in appropriate columns\n",
    "    df = df.append(pd.DataFrame({\"page\":name, \"title\":articles[1],\n",
    "                                 \"text\":articles[2], \"date\":articles[3],\n",
    "                                 \"url\":articles[0]}))\n",
    "\n",
    "# reset index and save to csv\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "df.to_csv(\"dataset.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
